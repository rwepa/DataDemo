"""
title   : iPAS-python-program
author  : Ming-Chang Lee
email   : alan9956@gmail.com
RWEPA   : http://rwepa.blogspot.tw/
Date    : 2020.11.18

Updated : 2022.01.01 -æ›´æ–° 08.ç¹ªåœ–ä¸­æ–‡å­—å‹
Updated : 2021.01.28 -æ–°å¢ 09.MySQLå¸¸ç”¨èªæ³•
Updated : 2021.01.28 -æ–°å¢ 10.Pythoné€£çµMySQL
Updated : 2021.02.17 -æ–°å¢ 11.Pythonç‰©ä»¶å°å‘
Updated : 2021.04.11 -æ–°å¢ 12.iPAS ç§‘ç›®äºŒï¼šè³‡æ–™è™•ç†èˆ‡åˆ†ææ¦‚è«–
Updated : 2021.08.09 -æ–°å¢ 13.æ±ºç­–æ¨¹ç¹ªåœ–4ç¨®æ–¹æ³•
Updated : 2021.09.30 -æ–°å¢ 14.æ·±åº¦å­¸ç¿’CNN - MNISTç¯„ä¾‹
Updated : 2022.01.16 -æ–°å¢ 15.Dashè¦–è¦ºåŒ–ç°¡ä»‹
Updated : 2022.02.12 -æ–°å¢ 16.Foliumåœ°ç†è¦–è¦ºåŒ–æ‡‰ç”¨
Updated : 2022.07.02 -æ–°å¢ 17.Orange3ç°¡ä»‹
Updated : 2022.08.02 -æ–°å¢ 18.condaè™›æ“¬ç’°å¢ƒ
Updated : 2022.09.21 -æ–°å¢ 19.ipynbè½‰æ›ç‚ºpdfæª”æ¡ˆ
"""

# ç¶“æ¿Ÿéƒ¨ iPAS å·¨é‡è³‡æ–™åˆ†æå¸«èªè­‰-Pythonå­¸ç¿’åƒè€ƒè³‡æ–™
# https://www.ipas.org.tw/bda/AbilityIndex.aspx

##############################
# å¤§ç¶±
##############################

# 00.conda,pipæ“ä½œ
# 01.è³‡æ–™å‹æ…‹èˆ‡åŸºæœ¬é‹ç®—
# 02.å­—ä¸²è™•ç†
# 03.Numpyè³‡æ–™çµæ§‹
# 04.Pandasè³‡æ–™çµæ§‹
# 05.æµç¨‹æ§åˆ¶èˆ‡ç‰©ä»¶å°å‘è§€å¿µ
# 06.è³‡æ–™è¼‰å…¥åŠåŒ¯å‡º
# 07.è³‡æ–™è®Šå½¢ã€æ’åºèˆ‡æ¸…ç†
# 08.æ¢ç´¢å¼è³‡æ–™åˆ†æ(å«ç¹ªåœ–ä¸­æ–‡å­—å‹)
# 09.MySQLå¸¸ç”¨èªæ³•
# 10.Pythoné€£çµMySQL
# 11.Pythonç‰©ä»¶å°å‘
# 12.iPAS - ç§‘ç›®äºŒï¼šè³‡æ–™è™•ç†èˆ‡åˆ†ææ¦‚è«–
# 13.æ±ºç­–æ¨¹ç¹ªåœ–4ç¨®æ–¹æ³•
# 14.æ·±åº¦å­¸ç¿’CNN - MNISTç¯„ä¾‹
# 15.Dashè¦–è¦ºåŒ–ç°¡ä»‹
# 16.Foliumåœ°ç†è¦–è¦ºåŒ–æ‡‰ç”¨
# 17.Orange3ç°¡ä»‹
# 18.condaè™›æ“¬ç’°å¢ƒ
# 19.ipynbè½‰æ›ç‚ºpdfæª”æ¡ˆ

# anaconda
# https://www.anaconda.com/

##############################
# 00.conda,pipæ“ä½œ
##############################

# conda è³‡è¨Š
conda info

# é¡¯ç¤ºå·²å®‰è£å¥—ä»¶
conda list
pip list

# æŸ¥è©¢æ¨¡çµ„è³‡è¨Š
pip show æ¨¡çµ„åç¨±

# å®‰è£æ¨¡çµ„
conda install æ¨¡çµ„åç¨±
pip install æ¨¡çµ„åç¨±

# æ›´æ–°æ¨¡çµ„
conda update æ¨¡çµ„åç¨±
pip install -U æ¨¡çµ„åç¨±

# ç¯„ä¾‹: æ›´æ–° anaconda æ¨¡çµ„
conda update anaconda

# ç¯„ä¾‹: æ›´æ–° Spyder æ¨¡çµ„
conda update spyder

# ç§»é™¤æ¨¡çµ„
conda remove æ¨¡çµ„åç¨±
pip uninstall æ¨¡çµ„åç¨±

##############################
# åˆ‡æ›å·¥ä½œç›®éŒ„
##############################
import os # è¼‰å…¥ os å¥—ä»¶
os.getcwd() # è®€å–å·¥ä½œç›®éŒ„
os.chdir("C:/pythondata") # è®Šæ›´å·¥ä½œç›®éŒ„
os.getcwd()
os.listdir(os.getcwd()) # é¡¯ç¤ºæª”æ¡ˆæ¸…å–®

# é¡¯ç¤ºæ¨¡çµ„æä¾›ä¹‹å‡½æ•¸
dir(os)

# jupyter notebook â€“ æ›´æ”¹é è¨­ç›®éŒ„
"""
cd C:\
jupyter-notebook
"""

# Jupyter Notebook è¡Œè™Ÿé‡æ–°è¨ˆç®—
# Kernel / Restart & Run All

# Pythonç¨‹å¼è¨­è¨ˆ-ææ˜æ˜Œ.ipynb
# http://rwepa.blogspot.com/2020/02/pythonprogramminglee.html

# Tools \ Preferences \ Keyboard shortcuts
# editor: run slection: F9        --> Ctrl + Return
# editor: run cell: Ctrl + Return --> F9

##############################
# 01.è³‡æ–™å‹æ…‹èˆ‡åŸºæœ¬é‹ç®—.py
##############################

# è®Šæ•¸

# åˆæ³•
å¤§æ•¸æ“š = 1 # ä¸­æ–‡äº¦å¯,å»ºè­°ä¸è¦ä½¿ç”¨
CustomerSaleReport = 1
_CustomerSaleReport = 1
Customer_Sale_Report = 1
customer_sale_report = 1

# ä¸åˆæ³•
$CustomerSaleReport = 1 # SyntaxError: invalid syntax
2020_sale = 100 # SyntaxError: invalid decimal literal
break = 123 # SyntaxError: invalid syntax

# å…§å»ºä¿ç•™å­—
dir(__builtins__)
len(dir(__builtins__)) # 159

# è³‡æ–™å‹æ…‹
# https://docs.python.org/3/library/stdtypes.html

# è³‡æ–™å‹æ…‹ â€“ ç¯„ä¾‹

# æ•´æ•¸ int
x1 = 1
type(x1)

# æµ®é»æ•¸ float
x2 = 1.234
type(x2)

# è¤‡æ•¸  complex
x3 = 1+2j
type(x3)

# å¸ƒæ—å€¼ (Boolean)
x4 = True
type(x4)
x4 + 10

# é‹ç®—å­
3 + 5
3 + (5 * 4)
3 ** 2
"Hello" + "World"
1 + 1.234
7 / 2
7 // 2
7 % 2
2 ** 10
1.234e3 - 1000

x5 = 1 == 2
x5
x5 + 10

# ä½ç§»é‹ç®—å­: << å‘å·¦ä½ç§»
# ä½ç§»é‹ç®—å­: >> å‘å³ä½ç§»
a = 4 << 3 # 0100 --> 0100000, 64 32 16 8 4 2 0
b = a * 4.5
c = (a+b)/2.5
a = "Hello World"

# Tuples åºåˆ— (å…ƒçµ„)

f = (2,3,4,5) # A tuple of integers
# g = (,) 	 # error code
g = ()
h = (2, [3,4], (10,11,12)) 	# A tuple containing mixed objects

# Tuplesçš„æ“ä½œ
x = f[1] # Element access. x = 3
x

y = f[1:3] # Slices. y = (3,4)
y

z = h[1][1] 	# Nesting. z = 4
z

xy = (2, 3)
xy

personal = ('Hannah',14,5*12+6)
personal

singleton = ("hello",)
singleton

# Tuple Operations
("chapter",8) + ("strings","tuples","lists")

2*(3,"blind","mice")

# single format: tuple[index]
# index : 0  ~  len(tuple)-1
# index: -len(tuple)  ~  -1
f= (2,3,4,5)
f[0]
f[-1] # ç´¢å¼• -1 è¡¨ç¤ºå€’æ•¸ç¬¬1å€‹å…ƒç´ 
f[-2]
f[len(f)-1]

# slice format: tuple [start:end ]. Items from start to (end -1)
t=((1,2), (2,"Hi"), (3,"RWEPA"), 2+3j, 6E23)
t[2]
t[:3]
t[3:]
t[-1]
t[-3:]

# Tuple Comparison Operations
# standard comparisons â€˜<â€™, â€˜<=â€™, â€˜>â€™, â€˜>=â€™, â€˜==â€™, â€˜!=â€™, in, not in

# Lists ä¸²åˆ—
# ä»»æ„ç‰©ä»¶çš„ä¸²åˆ—
a = [2, 3, 4]            # A list of integer
b = [2, 7, 3.5, "Hello"] # A mixed list
c = []	                 # An empty list
d = [2, [a, b]]	         # A list containing a list

len(d)
d[0]
d[1]

# ä¸²åˆ—çš„æ“ä½œ
x = a[1] 	   # Get 2nd element (0 is first)
y = b[1:3] 	   # Return a sub-list
z = d[1][0][2] # Nested lists
b[0] = 42 	   # Change an element

# ä¸²åˆ—çš„çµåˆèˆ‡é‡è¤‡
e = a + b			     # Join two lists
e

f = e*3                  # repeat lists
f

# é™„åŠ å…ƒç´ 
# append 
a.append('BigData')
a

# extend
a.extend(['Python', 'R', "Julia"])
a

# æ’å…¥å…ƒç´ 
a.insert(2, 999)
a

# åˆªé™¤å…ƒç´ 
del a[4]
a

del a
print(a)

# é¡¯ç¤ºæ–¹æ³•
print(dir(list))

# Set é›†åˆ
# https://docs.python.org/3/tutorial/datastructures.html#sets
# é›†åˆèˆ‡å­—å…¸ç›¸ä¼¼, ä½†å­—å…¸æ²’æœ‰key,åªæœ‰å€¼
a = set() 			# An empty set
type(a)

b = {"å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "æ¡ƒåœ’å¸‚", "å°ä¸­å¸‚", "å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "é«˜é›„å¸‚"}
b # {'å°ä¸­å¸‚', 'å°åŒ—å¸‚', 'æ–°åŒ—å¸‚', 'æ¡ƒåœ’å¸‚', 'é«˜é›„å¸‚'}

# é›†åˆé‹ç®—
x = {1,2,3,4,5}
y = {1,3,5,7}
x & y # {1, 3, 5}
x | y # {1, 2, 3, 4, 5, 7}
x ^ y # {2, 4, 7}

# Dictionaries å­—å…¸
a = {}  # An empty dictionary
type(a) # dict
b = { 'x': 3, "y": 4 }
c = { "uid": 168,
	   "login": "marvelous",
	   "name" : 'Alan Lee'
	 }

# Get all keys
c.keys()

# Get an element
u = c["uid"]

# Add an element
c["shell"] = "/bin/sh"

if c.has_key("directory"): 	# 'dict' object has no attribute 'has_key'
	d = c["directory"]
else:
	d = None
 
if "directory" in c:   # v3.x ç›´æ¥ä½¿ç”¨ in
    d = c["directory"]
else:
    d = None
    
if "uid" in c:         # v3.x ç›´æ¥ä½¿ç”¨ in
    d = c["uid"]
else:
    d = None

d = c.get("directory", None) # è¼ƒç°¡æ½”

d = c.get("uid", None) # è¼ƒç°¡æ½”

##############################
# 02.å­—ä¸²è™•ç†
##############################

# å­—ä¸² (String)ç”±ä¸€è¨±å¤šå­—å…ƒæ‰€çµ„æˆ
# å­—ä¸²å·¦å³äºŒå´é ˆä½¿ç”¨å–®å¼•è™Ÿæˆ–æ˜¯é›™å¼•è™Ÿ

# å­—ä¸²ç‰©ä»¶ https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str

# å­—ä¸²æ–¹æ³• https://docs.python.org/3/library/string.html

city = 'è‹—æ —ç¸£'
district = 'é€ æ©‹é„‰'
road = 'å­¸åºœè·¯168è™Ÿ'

# ä½¿ç”¨ + å­—ä¸²ä¸²æ¥
city + district

address = city + district + road
address

# æ•¸å€¼è½‰æ›ç‚ºå­—ä¸²
myinteger = 123
mystr = str(myinteger)
mystr

# len é•·åº¦
len(address)

# split åˆ†å‰²å­—ä¸²
address = city + ',' + district + ',' + road
address
address.split(',')

# replace å–ä»£å­—ä¸²
address.replace(',', '')

# find æœå°‹
mystr = 'Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning.'

# find å›å‚³å­—ä¸²çš„ç´¢å¼•, æ‰¾ä¸åˆ°å›å‚³-1
mystr.find('learning') # 39
mystr.find('bigdata') # -1

mystr.index('learning') # 39
mystr.index('bigdata') # ValueError: substring not found

# rfind å¾æœ€å¾Œé¢æŸ¥è©¢
mystr.rfind('learning') # 98

##############################
# 03.Numpyè³‡æ–™çµæ§‹
##############################

# NumPy æ¨¡çµ„
import numpy as np

# ä½¿ç”¨ä¸²åˆ—èˆ‡å…ƒçµ„å»ºç«‹ä¸€ç¶­é™£åˆ—
a = np.array([1, 2, 3, 4, 5])
b = np.array((1, 2, 3, 4, 5))

print(type(a))
print(type(b))

print(a[0], a[1], a[2], a[3])

b[0] = 5    
print(b) 

b[4] = 0
print(b)

# ä½¿ç”¨å·¢ç‹€æ¸…å–®å»ºç«‹äºŒç¶­é™£åˆ—
a = np.array([[1,2,3],[4,5,6]])
print(type(a))

print(a[0, 0], a[0, 1], a[0, 2])

print(a[1, 0], a[1, 1], a[1, 2])

a[0, 0] = 6
a[1, 2] = 1
print(a)

# ä½¿ç”¨ save å°‡Numpyé™£åˆ—å„²å­˜æˆå¤–éƒ¨æª”æ¡ˆ
outputfile = 'myarray.npy'
with open(outputfile, 'wb') as fp:
    np.save(fp, a)

# ä½¿ç”¨ load å°‡å¤–éƒ¨æª”æ¡ˆåŒ¯å…¥è‡³Numpyé™£åˆ—
outputfile = "myarray.npy"
with open(outputfile, 'rb') as fp:
    mydata = np.load(fp)
print(mydata)

# äº‚æ•¸å‡½æ•¸

# è¨­å®šäº‚æ•¸å‡½æ•¸çš„ç¨®å­, é ˆè¼¸å…¥ >= 1 çš„æ•´æ•¸
np.random.seed(123)

# random() ç”¢ç”Ÿ0.0~1.0ä¹‹é–“çš„äº‚æ•¸
x1 = np.random.random()
x2 = np.random.random()
print(x1,x2)

# randint(min, max, size) ç”¢ç”Ÿmin~maxä¹‹é–“çš„æ•´æ•¸äº‚æ•¸, ä¸å«max
# https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html

x3 = np.random.randint(5, 10)
x4 = np.random.randint(1, 101)
print(x3, x4)

x5 = np.random.randint(5, 21, size=3)
print(x5)

x6 = np.random.randint(1, 11, size=(4, 5))
print(x6)

# rand() ç”¢ç”Ÿäº‚æ•¸å€¼çš„é™£åˆ—
a = np.random.rand(5)
print(a)

b = np.random.rand(5, 4)  
print(b)

# å¸¸æ•¸ Constants
np.NAN

# NaN and NAN are equivalent definitions of nan. 
# Please use nan instead of NAN.
# æ–°ç‰ˆæœ¬ä½¿ç”¨ nan
np.nan

np.pi

np.e

a = np.array([30, 45, 60, 90])
np.sin(a*np.pi/180)

# é™£åˆ—çš„å±¬æ€§

# reshape æ‡‰ç”¨1
a = np.array([0,1,2,3,4,5])
a
a.ndim   # 1
a.shape  # (6,)

# å»ºç«‹å‰¯æœ¬, bä¹‹ä¿®æ”¹æœƒå½±éŸ¿a
b = a.reshape((3,2))
b
b.ndim   # 2
b.shape  # (3,2)

b[1][0] = 168
b
a # aç‰©ä»¶å·²ç¶“æ›´æ”¹, array([0, 1, 168, 3, 4, 5])

c = a.reshape((3,2)).copy()
c
c[0][0] = -999
c
a # aç‰©ä»¶æ²’æœ‰æ›´æ”¹

# reshape æ‡‰ç”¨2
a = np.array([[1, 2, 3], [4, 5, 6]])
a

b= a.reshape((3, 2))
b

b[1, 0] = 999
b
a

c = b.reshape(-1, 3)
c

# reshape æ‡‰ç”¨3
z = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
z
z.reshape(-1) # -1: unknown dimension
# array([ 1,  2,  3, ..., 10, 11, 12])

z.reshape(-1,1) # row -1: unknown , column 1

z.reshape(-1, 2) # row -1: unknown , column 2

z.reshape(1,-1) # row 1 , column: unknown

z.reshape(2, -1) # row 2 , column: unknown

z.reshape(3, -1) # row 3 , column: unknown

z.reshape(-1, -1) # ERROR

# å‘é‡åŒ–è™•ç†
a = np.array([0,1,1,2,3,5])

a*2

a**3 # æ¬¡æ–¹é‹ç®—

# indexing

a[np.array([1,3,5])]

# é›¢ç¾¤å€¼èª¿æ•´
a = a**3
a > 10
a[a > 10]
a[a > 10] = 10
a

a.clip(0, 3)

# nan è™•ç†
x = np.array([1, 2, 3, np.NAN, 4])
x
np.isnan(x)

x[~np.isnan(x)]

np.mean(x[~np.isnan(x)])

np.mean(x) # nan

# è¨ˆç®—æ™‚é–“
import timeit
import numpy as np

normal_py_sec = timeit.timeit('sum(x*x for x in range(1000))', number=10000)
naive_np_sec = timeit.timeit('sum(na*na)', setup="import numpy as np; na=np.arange(1000)", number=10000)
good_np_sec = timeit.timeit('na.dot(na)', setup="import numpy as np; na=np.arange(1000)", number=10000)

print("Normal Python: %f sec"%normal_py_sec)
print("Naive NumPy: %f sec"%naive_np_sec)
print("Good NumPy: %f sec"%good_np_sec)

print "Hello World"    # python 2, Python 3 with ERROR
print("Hello World")   # python 3

##############################
# 04.Pandasè³‡æ–™çµæ§‹
##############################

# pandas è¨­å®šé¡¯ç¤ºæ‰€æœ‰æ¬„ä½
pd.set_option('display.expand_frame_repr', False)

# è¼‰å…¥2å¤§å¥—ä»¶ pandas, numpy
# https://pandas.pydata.org/docs/user_guide/10min.html

import pandas as pd # Python Data Analysis Library
import numpy as np # Python Scientific Computing Library 

# åºåˆ—(Series)ç‰©ä»¶
# ä½¿ç”¨ä¸²åˆ—(List)å»ºç«‹åºåˆ—ç‰©ä»¶
# åºåˆ—åŒ…æ‹¬æŒ‡æ¨™(Index) èˆ‡å€¼(Value), æŒ‡æ¨™æ¡ç”¨é è¨­æ•´æ•¸å‹æ…‹æŒ‡æ¨™
s = pd.Series([1,3,5,np.nan,6,8])
s
type(s)

# ä½¿ç”¨é™£åˆ—å»ºç«‹è³‡æ–™æ¡†(DataFrame)
dates = pd.date_range('20200801', periods=6) # æ—¥æœŸæŒ‡æ¨™
dates
type(dates)

# è³‡æ–™æ¡†(DataFrame)ç‰©ä»¶
# æ¬„ä½åç¨±: A, B, C, D
df1 = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
df1
type(df1)

# ä½¿ç”¨å­—å…¸å»ºç«‹è³‡æ–™æ¡† DataFrame
df2 = pd.DataFrame({ 'A' : 1.,
    'B' : pd.Timestamp('20190101'),
    'C' : pd.Series(1,index=list(range(4)),dtype='float32'),
    'D' : np.array([3] * 4,dtype='int32'),
    'E' : pd.Categorical(["test","train","test","train"]),
    'F' : 'foo' })
df2

# dtypes: è¡¨ç¤ºè³‡æ–™å‹æ…‹
df2.dtypes # df2. æŒ‰ [Tab] æŒ‰éˆ•

# Viewing Data è³‡æ–™æª¢è¦–
df1

# head é¡¯ç¤ºå‰ 5 ç­†è³‡æ–™, æ­¤åŠŸèƒ½èˆ‡ R é¡¯ç¤º 6 ç­†ä¸ç›¸åŒ.
df1.head()

df1.head(3)

df1.tail()

# é¡¯ç¤ºæŒ‡æ¨™(index)
df1.index

# æ¬„åç¨±(columns)
df1.columns

# è³‡æ–™å€¼(values)
df1.values

# describe çµ±è¨ˆæ‘˜è¦(statistic summary)
# count å€‹æ•¸
# mean å¹³å‡å€¼
# std  æ¨™æº–å·® standard deviation, ä¸€èˆ¬å¸Œæœ›æ„ˆå°æ„ˆå¥½
# min  æœ€å°å€¼
# 25%  25ç™¾åˆ†ä½æ•¸
# 50%  50ç™¾åˆ†ä½æ•¸, ä¸­ä½æ•¸ median
# 75%  75ç™¾åˆ†ä½æ•¸ (quantile)
# max  æœ€å¤§å€¼

df1.describe()

# æ’åº sort_values()
# axisç‚ºæ’åºçš„è»¸ï¼Œ0è¡¨ç¤º rows index(åˆ—æŒ‡æ¨™)ï¼Œ1è¡¨ç¤ºcolumns index(è¡ŒæŒ‡æ¨™)
# ç•¶å°æ•¸æ“š "åˆ—" é€²è¡Œæ’åºæ™‚ï¼Œaxiså¿…é ˆè¨­ç½®ç‚º0.
# df.sort(["A"]) æ–°ç‰ˆä¸æ”¯æ´ sort, æ”¹ç”¨ sort_values æˆ– sort_index

# ascending =FALSE, å³éå¢æ˜¯FALSE, è¡¨ç¤ºéæ¸›æ˜¯TRUE, çµæœç‚ºD,C,B,A
df1.sort_index(axis=1, ascending=False)

# ä¾ç…§ B æ¬„å¤§å°, ç”±å°è‡³å¤§æ’åº
df1.sort_values(by='B')

# é¸å–è¡Œ
df1['A']
df1.A

# é¸å–åˆ—, æ­¤åŠŸèƒ½èˆ‡ R ä¸åŒ, R: df[1:4] è¡¨ç¤ºé¸å–ç¬¬1è‡³ç¬¬4è¡Œ
df1
df1[1:4]

# ä½¿ç”¨ loc
df1.loc[:, ['A','B']]

# ä½¿ç”¨ iloc
df1.iloc[2] # æŒ‡æ¨™ç‚ºç¬¬2åˆ—

df1.iloc[2:4,]
df1.iloc[2:4, :]

df1.iloc[, 2] # ERROR
df1.iloc[:, 2] # OK
df1.iloc[:, 2:4] 

# å–å‡ºä¸€å€‹å„²å­˜æ ¼ cell, ä½¿ç”¨ at æˆ– iat

# at ä½¿ç”¨è®Šæ•¸åç¨±
df2.at[0,'A']

# iat ä½¿ç”¨æŒ‡æ¨™ index
df2.iat[1,4]

# Boolean Indexing é‚è¼¯å€¼(æ¢ä»¶å¼)è³‡æ–™é¸å–
df1[df1 > 0]
df1[df1.A > 0]

# ä½¿ç”¨ .isin
df1[df1.index.isin(['2020-08-02', '2020-08-04'])]

# å»ºç«‹è³‡æ–™æ¡†
df2 = df1.reindex(index=dates[0:4], columns=list(df1.columns) + ['E'])
df2

# Missing Data éºæ¼å€¼ NaN, R: ä½¿ç”¨NA
df2.loc[dates[0]:dates[1],'E'] = 1
df2

# åˆªé™¤åˆ—ä¸­åŒ…æ‹¬ NaN
df2.dropna(how='any')

# å°‡éºæ¼å€¼å¡«å…¥å€¼
df2.fillna(value=999)

# åˆ¤æ–·ä½•è€…ç‚ºNaN
pd.isnull(df2)

# è¨ˆç®—æ¯è¡Œå¹³å‡
df2.mean()

# è¨ˆç®—æ¯åˆ—å¹³å‡
df2.mean(1)

# Apply å°‡è³‡æ–™å¥—ç”¨è‡³å‡½æ•¸
df2.apply(np.cumsum)

# Merge åˆä½µ
df = pd.DataFrame(np.random.randn(10, 4))
df

pieces = [df[:3], df[4:7], df[8:]]
pieces

# åˆ—åˆä½µ, é¡ä¼¼ Rçš„ rbind
pd.concat(pieces)

# Grouping ç¾¤çµ„è¨ˆç®—
df = pd.DataFrame({
    'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],
    'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],
    'C' : np.random.randn(8),
    'D' : np.random.randn(8)})
df

df.groupby('A').sum() # é¡ä¼¼ R- aggregate

df.groupby(['A','B']).sum()

# ç¹ªåœ–
ts = pd.Series(np.random.randn(1000), 
               index=pd.date_range('1/1/2000', periods=1000))
ts

ts = ts.cumsum()

ts.plot()

##############################
# 05.æµç¨‹æ§åˆ¶èˆ‡ç‰©ä»¶å°å‘è§€å¿µ
##############################

# elifæ•˜è¿°
a = '+'

if a == '+':
	op = 'PLUS'
elif a == '-':
	op = 'MINUS'
else:
	op = 'UNKNOWN'

op

# æ²’æœ‰åƒCèªè¨€ä¸€æ¨£ï¼Œæœ‰switchçš„èªæ³•
# å¸ƒæ—è¡¨ç¤ºå¼ â€“ and, or, not
a = 1
b = 6
c = 9

if b >= a and b <= c:
	print('b is between a and c')
    
if not (b < a or c > c):
	print('b is still between a and c')

#================
# Functions å‡½æ•¸
#================
# Return the remainder of a/b
def remainder(a,b):
	q = a/b
	r = a - q*b
	return r

# Now use it
a = remainder(42,5) 	# a = 2

# Return two values
def divide(a,b):
	q = a/b
	r = a - q*b
	return q,r

x,y = divide(42,5) 		# x = 8, y = 2

# æª”æ¡ˆè™•ç†

# 1.æª”æ¡ˆçš„è®€å–/å¯«å…¥
f = open("foo","w") 	# Open a file for writing
f.write("Hello World")
f.close()

g = open("foo","r") 	# Open a file for reading
data = g.read() 		# Read all data
data

g = open("foo","r")
line = g.readline() 	# Read a single line
line

g = open("foo","r")
lines = g.readlines() # Read data as a list of lines
lines

# 2.ä½¿ç”¨%ä¾†æ ¼å¼åŒ–å­—ä¸²
for i in range(0,10):
	f.write("2 times %d = %d\n" % (i, 2*i))

for i in range(0,10):
	print("2 times %d = %d\n" % (i, 2*i))

# ç‰©ä»¶å°å‘
# ç¯„ä¾‹1
# å»ºç«‹é¡åˆ¥ class
class MyClass:
  x = 5

# ä½¿ç”¨ () å¯¦ä½œå¯¦ä¾‹
p1 = MyClass()
print(p1.x)

# ç¯„ä¾‹2
# __init__() å‡½æ•¸è¡¨ç¤ºé¡åˆ¥åˆä½¿åŒ–æ™‚,è©²å‡½æ•¸æœƒè‡ªå‹•åŸ·è¡Œ, å·¦å³ç‚ºäºŒå€‹åº•ç·š
class Person:
  def __init__(self, name, age):
    self.name = name
    self.age = age

p1 = Person("John", 36)

p1

print(p1.name)
print(p1.age)  

# ç¯„ä¾‹3
# Inheritance ç¹¼æ‰¿
# çˆ¶é¡åˆ¥(Parent class)æ˜¯çµ¦å…¶ä»–ç¹¼æ‰¿çš„é¡åˆ¥, ç¨±ç‚ºåŸºæœ¬é¡åˆ¥.
# å­é¡åˆ¥(Child class )æ˜¯å¾å¦ä¸€å€‹çˆ¶é¡åˆ¥ç¹¼æ‰¿çš„, ä¹Ÿç¨±ç‚ºè¡ç”Ÿé¡åˆ¥(derived class).
# https://www.w3schools.com/python/python_inheritance.asp

# çˆ¶é¡åˆ¥
class Person:
  def __init__(self, fname, lname):
    self.firstname = fname
    self.lastname = lname

  def printname(self):
    print(self.firstname, self.lastname)

x = Person("ALAN", "LEE")
x.printname()

# class å­é¡åˆ¥(çˆ¶é¡åˆ¥)
class Student(Person):
  pass

x = Student("Mike", "Olsen")
x.printname()

# æ–°å¢ __init__ å‡½æ•¸
# å­é¡åˆ¥çš„__init __()å‡½æ•¸å°‡è¦†è“‹çˆ¶é¡åˆ¥çš„__init __()å‡½æ•¸
class Student(Person):
  def __init__(self, fname, lname):
    Person.__init__(self, fname, lname)

# super - è¡¨ç¤ºä½¿ç”¨çˆ¶é¡åˆ¥çš„æ–¹æ³•
class Student(Person):
  def __init__(self, fname, lname):
    super().__init__(fname, lname)

x = Student("Big", "Data")

x.printname()

##############################
# 06.è³‡æ–™è¼‰å…¥åŠåŒ¯å‡º
##############################

# pandas IO æ¨¡çµ„
# https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html

# åŒ¯å…¥ CSV æª”æ¡ˆ
import pandas as pd

# ä½¿ç”¨ [CTRL] + å·¦éµ --> Raw --> ä¸‹è¼‰ marketing.csv è‡³ C:\pythondata\data è³‡æ–™å¤¾
# https://github.com/rwepa/DataDemo/blob/master/marketing.csv

# åŒ¯å…¥è³‡æ–™
# åŠ ä¸Šè§’é¸å– C:\pythondata è³‡æ–™å¤¾
marketing = pd.read_csv('data/marketing.csv')
marketing # 200*4

# è³‡æ–™æ‘˜è¦
marketing.describe()

# æœ‰NaN
marketing.isnull().sum()

marketing['facebook']

# å°‡ facebook è®Šæ•¸çš„ NaNè³‡æ–™, ä»¥ä¸­ä½æ•¸å¡«æ»¿
marketing['facebook'].fillna(marketing['facebook'].median, inplace = True)

# æ²’æœ‰NaN
marketing.isnull().sum()

marketing

# åŒ¯å…¥ Excel æª”æ¡ˆ, å…¨åœ‹è¨‚å–®æ˜ç´°.xlsx
# https://github.com/rwepa/DataDemo/blob/master/å…¨åœ‹è¨‚å–®æ˜ç´°.xlsx

sales = pd.read_excel(io = 'data/å…¨åœ‹è¨‚å–®æ˜ç´°.xlsx', sheet_name = 'å…¨åœ‹è¨‚å–®æ˜ç´°')
sales # 8568*19
sales.head()

# åŒ¯å…¥ SAS æª”æ¡ˆ, h_nhi_ipdte103.sas7bdat
# è³‡æ–™èªªæ˜: 103å¹´æ¨¡æ“¬å…¨æ°‘å¥ä¿è™•æ–¹åŠæ²»ç™‚æ˜ç´°æª”_è¥¿é†«ä½é™¢æª”
# è³‡æ–™ç­†æ•¸: 14297
# æ¬„ä½å€‹æ•¸: 80
# æª”æ¡ˆå¤§å°: 7.25MB
# https://github.com/rwepa/DataDemo/blob/master/h_nhi_ipdte103.sas7bdat

ipdate = pd.read_sas(filepath_or_buffer = 'data/h_nhi_ipdte103.sas7bdat')
ipdate # 14297*80
ipdate.head()

# è³‡æ–™åŒ¯å‡º
df = pd.DataFrame({'å§“å': ['ALAN', 'LEE'],
                   'åœ°å€': ['å°åŒ—å¸‚', 'æ–°åŒ—å¸‚'],
                   'å¹´è³‡': [10, 20]})
df
#      å§“å   åœ°å€  å¹´è³‡
# 0  ALAN  å°åŒ—å¸‚  10
# 1   LEE  æ–°åŒ—å¸‚  20

df.to_csv('data/df.csv', index = False) # Windows ä¸­æ–‡äº‚ç¢¼

df.to_csv('data/df.csv', index = False, encoding = 'utf-8') # ä¸­æ–‡äº‚ç¢¼

df.to_csv('data/df.csv', index = False, encoding = 'utf_8_sig') # OK

##############################
# 07.è³‡æ–™è®Šå½¢,æ’åºèˆ‡æ¸…ç†
##############################

import pandas as pd
import numpy as np

# pandas.DataFrame.pivot
# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html

df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two', 'two'],
                   'bar': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'baz': [1, 2, 3, 4, 5, 6],
                   'zoo': ['x', 'y', 'z', 'q', 'w', 't']})
df
#    foo bar  baz zoo
# 0  one   A    1   x
# 1  one   B    2   y
# 2  one   C    3   z
# 3  two   A    4   q
# 4  two   B    5   w
# 5  two   C    6   t

# DataFrame.pivot(index=None, columns=None, values=None)
df.pivot(index='foo', columns='bar', values='baz')
# bar  A  B  C
# foo         
# one  1  2  3
# two  4  5  6

df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])
#     baz       zoo      
# bar   A  B  C   A  B  C
# foo                    
# one   1  2  3   x  y  z
# two   4  5  6   q  w  t

# pandas.DataFrame.pivot_table
# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html

df = pd.DataFrame({"A": ["foo", "foo", "foo", "foo", "foo", "bar", "bar", "bar", "bar"],
                   "B": ["one", "one", "one", "two", "two", "one", "one", "two", "two"],
                   "C": ["small", "large", "large", "small", "small", "large", "small", "small", "large"],
                   "D": [1, 2, 2, 3, 3, 4, 5, 6, 7],
                   "E": [2, 4, 5, 5, 6, 6, 8, 9, 9]})
df
#      A    B      C  D  E
# 0  foo  one  small  1  2
# 1  foo  one  large  2  4
# 2  foo  one  large  2  5
# 3  foo  two  small  3  5
# 4  foo  two  small  3  6
# 5  bar  one  large  4  6
# 6  bar  one  small  5  8
# 7  bar  two  small  6  9
# 8  bar  two  large  7  9

table = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)
table
# C        large  small
# A   B                
# bar one    4.0    5.0
#     two    7.0    6.0
# foo one    4.0    1.0
#     two    NaN    6.0

# è³‡æ–™è®Šå½¢-å¯¬è³‡æ–™è½‰æ›ç‚ºé•·è³‡æ–™

df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
                   'B': {0: 1, 1: 3, 2: 5},
                   'C': {0: 2, 1: 4, 2: 6}})
df
#    A  B  C
# 0  a  1  2
# 1  b  3  4
# 2  c  5  6

# è­˜åˆ¥è®Šæ•¸ç‚ºA,è½‰æ›å€¼è®Šæ•¸ç‚ºB
df.melt(id_vars=['A'], value_vars=['B'])
#    A variable  value
# 0  a        B      1
# 1  b        B      3
# 2  c        B      5

# è­˜åˆ¥è®Šæ•¸ç‚ºA,è‡ªè¨‚è½‰æ›å¾Œè®Šæ•¸åç¨±
df.melt(id_vars=['A'], value_vars=['B'],
        var_name='myVarname', value_name='myValname')
#    A myVarname  myValname
# 0  a         B          1
# 1  b         B          3
# 2  c         B          5

# è­˜åˆ¥è®Šæ•¸ç‚ºA,è½‰æ›å€¼è®Šæ•¸ç‚ºB, C
longdf = df.melt(id_vars=['A'], value_vars=['B', 'C'])
longdf
#    A variable  value
# 0  a        B      1
# 1  b        B      3
# 2  c        B      5
# 3  a        C      2
# 4  b        C      4
# 5  c        C      6

# è³‡æ–™è®Šå½¢-é•·è³‡æ–™è½‰æ›ç‚ºå¯¬è³‡æ–™
longdf.pivot(index='A', columns='variable', values='value')
# variable  B  C
# A             
# a         1  2
# b         3  4
# c         5  6

# æ’åº-é è¨­å€¼éå¢
longdf.sort_values('value')
#    A variable  value
# 0  a        B      1
# 3  a        C      2
# 1  b        B      3
# 4  b        C      4
# 2  c        B      5
# 5  c        C      6

# æ’åº-éæ¸›
longdf.sort_values('value', ascending=False)
#    A variable  value
# 5  c        C      6
# 2  c        B      5
# 4  b        C      4
# 1  b        B      3
# 3  a        C      2
# 0  a        B      1

# æ’åº-2å€‹ä»¥ä¸Šæ¬„ä½
longdf.sort_values(['A','variable','value'])
#    A variable  value
# 0  a        B      1
# 3  a        C      2
# 1  b        B      3
# 4  b        C      4
# 2  c        B      5
# 5  c        C      6

# å…§æ’æ³• interpolate
# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html

# ç¯„ä¾‹1
s = pd.Series([0, 1, np.nan, 2])
s
# 0    0.0
# 1    1.0
# 2    NaN
# 3    2.0
# dtype: float64

s.interpolate()
# 0    0.0
# 1    1.0
# 2    1.5
# 3    2.0
# dtype: float64

# ç¯„ä¾‹2 - è¨­å®šæœ€å¤šå¡«è£œäºŒå€‹é€£çºŒ NaN
s = pd.Series([np.nan, "single_one", np.nan,
               "fill_two_more", np.nan, np.nan, np.nan,
               4.71, np.nan])
s
# 0              NaN
# 1       single_one
# 2              NaN
# 3    fill_two_more
# 4              NaN
# 5              NaN
# 6              NaN
# 7             4.71
# 8              NaN
# dtype: object

s.interpolate()

s.interpolate(method='pad', limit=2) # padç‚ºæœ€è¿‘å€¼
# 0              NaN
# 1       single_one
# 2       single_one
# 3    fill_two_more
# 4    fill_two_more
# 5    fill_two_more
# 6              NaN
# 7             4.71
# 8             4.71
# dtype: object

# ç¯„ä¾‹3 - ä½¿ç”¨å¤šé …å¼æ³•, åƒè€ƒ scipyæ¨¡çµ„
s = pd.Series([0, 2, np.nan, 8])
s
# 0    0.0
# 1    2.0
# 2    NaN
# 3    8.0

s.interpolate(method='polynomial', order=2)
# 0    0.000000
# 1    2.000000
# 2    4.666667
# 3    8.000000
# dtype: float64

##############################
# 08.æ¢ç´¢å¼è³‡æ–™åˆ†æ
##############################

# matplotlib
import matplotlib.pyplot as plt
import numpy as np

N = 50
x = np.random.rand(N)
y = np.random.rand(N)
colors = np.random.rand(N)
area = np.pi * (15 * np.random.rand(N))**2  # åŠå¾‘ 0~15

# æ•£ä½ˆåœ– plt.scatter
plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.savefig('random.plot.png')
plt.savefig('random.plot.pdf')
plt.show()

plt.scatter(x, y, s=500, c=colors, alpha=0.5)
plt.show()

# ç·šåœ– plt.plot
plt.plot(x,y)

plt.plot(sorted(x), y)

# example 1 - ç·šåœ–
plt.plot([1,2,3,4])   # x è»¸: 0,1,2,3
plt.ylabel("Quality")
plt.show()

# example 2 - é»åœ–
plt.plot([1,2,3,4], [1,4,9,16], 'ro')  # r:red, o:circle marker
plt.axis([0, 6, 0, 20]) # set xlim and ylim
plt.show()

# example 3 - 3çµ„è³‡æ–™
t = np.arange(0., 5., 0.2) # ç­‰å·®ç´šæ•¸, å€é–“åŒ…æ‹¬å•Ÿå§‹, ä¸åŒ…æ‹¬çµæŸ
# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()

# example 4
plt.figure(1)                # the first figure

# 211: (nrow, ncol, plot_number) 
plt.subplot(211)             # the first subplot in the first figure
plt.plot([1, 2, 3])
plt.subplot(212)             # the second subplot in the first figure
plt.plot([4, 5, 6])

plt.figure(2)                # a second figure
plt.plot([4, 5, 6])          # creates a subplot(111) by default

plt.figure(1)                # figure 1 current; subplot(212) still current
plt.subplot(211)             # make subplot(211) in figure1 current
plt.title('Easy as 1, 2, 3') # subplot 211 title

# example 5
def f(t):
    return np.exp(-t) * np.cos(2*np.pi*t)

t1 = np.arange(0.0, 5.0, 0.1)
t2 = np.arange(0.0, 5.0, 0.02)

plt.figure(1)
plt.subplot(211)
plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')

plt.subplot(212)
plt.plot(t2, np.cos(2*np.pi*t2), 'r--')
plt.show()

# example 6 ç›´æ–¹åœ– (histogram)
import numpy as np
import matplotlib.pyplot as plt

# Fixing random state for reproducibility
np.random.seed(123)

mu, sigma = 100, 15
x = mu + sigma * np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, density=True, facecolor='g', alpha=0.75)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title('Histogram of IQ')
plt.text(50, 0.025, r'$\mu=100,\ \sigma=15$', size = 50)
plt.xlim(40, 160)
plt.ylim(0, 0.03)
plt.grid(True)
plt.show()

# example 7 å¤šåˆ—å¤šè¡Œç¹ªåœ–
fig = plt.figure()

# subplot:è¨­å®šåœ–å½¢ä½ç½®(åˆ—,è¡Œ,ç·¨è™Ÿ)
# fig.add_subplot(221)   #top left
# fig.add_subplot(222)   #top right
# fig.add_subplot(223)   #bottom left
# fig.add_subplot(224)   #bottom right 
# plt.show()

# plt.subplot()
# subplot(232) # 2åˆ—, 3è¡Œ, åœ–2ä½ç½®
# åœ–1 , åœ–2 , åœ–3
# åœ–4 , åœ–5 , åœ–6

# 08.ç¹ªåœ–ä¸­æ–‡å­—å‹
# example 8 Windows ä¸­æ–‡å­—å‹è§£æ±ºæ–¹æ¡ˆ
import os
os.getcwd()
os.chdir("C:/pythondata")

import matplotlib.pyplot as plt
import numpy as np

##############################
# Windows ä¸­æ–‡å­—å‹
##############################

##############################
# æ–¹æ³•1
##############################

# (1).é—œé–‰Spyder

# (2).å°‡å¾®è»Ÿæ­£é»‘é«” C:\Windows\fonts\msjh.ttc è¤‡è£½åˆ°ä»¥ä¸‹ç›®éŒ„
# ã€C:\Users\user\anaconda3\Lib\site-packages\matplotlib\mpl-data\fonts\ttfã€‘

# (3).å°‡ç¾æœ‰ DejaVuSans.ttf æ›´åç‚º DejaVuSans-old.ttf

# (4).å°‡ msjh.ttc æ›´åç‚º DejaVuSans.ttf

# (5).é‡æ–°å•Ÿå‹•Spyderå³å¯ä½¿ç”¨ä¸­æ–‡å­—å‹

##############################
# æ–¹æ³•2 ä½¿ç”¨  matplotlib.font_manager.FontProperties æ–¹æ³•
##############################

from matplotlib.font_manager import FontProperties

font = FontProperties(fname=r"c:\windows\fonts\mingliu.ttc", size=12)

# åŒ¯å…¥è³‡æ–™
# https://github.com/rwepa/DataDemo/blob/master/web_traffic.csv
myData = np.genfromtxt("web_traffic.csv", delimiter = '\t')

print(myData[:6])
myData.ndim # 2å€‹ç¶­åº¦
myData.shape # same as print(myData.shape) 743*2

x = myData[:,0] # 743*1
y = myData[:,1] # 743*1

# æª¢æŸ¥æ˜¯å¦æœ‰Na
np.sum(np.isnan(y)) # æœ‰8å€‹å€¼æ˜¯nan
print("Number of invalid entries:", np.sum(np.isnan(y)))

# å–å‡ºéNa
x = x[~np.isnan(y)] # 743-8=735
y = y[~np.isnan(y)]

# ç¹ªåœ–
plt.scatter(x, y, s=10)
plt.title(u"2017å¹´10æœˆæ¯å°æ™‚ç¶²è·¯æµé‡", fontproperties=font) # ä¸­æ–‡é¡¯ç¤º
plt.xlabel("Time")
plt.ylabel("Hits/hour")
plt.xticks([w*7*24 for w in range(10)], ['week %i' % w for w in range(10)])
plt.autoscale(tight=True)

# åŠ ä¸Šç¶²æ ¼ç·š
plt.grid(True, linestyle='-', color='0.75')

# è¼¸å‡º png
plt.savefig(u"2017å¹´10æœˆæ¯å°æ™‚ç¶²è·¯æµé‡.png", dpi=300, format="png")
plt.show()

##############################
# æ–¹æ³•3 ä½¿ç”¨ matplotlib.rcParams æ–¹æ³•
##############################

import matplotlib

# è¨­å®š matplotlib.rcParams æ–¹æ³•
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei']

# è¨­å®šè² è™ŸéŒ¯èª¤é¡¯ç¤º
matplotlib.rcParams['axes.unicode_minus'] = False

##############################
# Mac ä¸­æ–‡å­—å‹
##############################

# import matplotlib
# print(matplotlib.matplotlib_fname())
# /Users/rwepa/opt/anaconda3/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc

# ç·¨è¼¯ matplotlibrc, æœ€åº•ä¸‹æ–°å¢3è¡ŒæŒ‡ä»¤
# font.family: sans-serif
# font.sans-serif: Arial Unicode MS, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif
# axes.unicode_minus: False

# åˆªé™¤æš«å­˜æª”
# rm -rf ~/.matplotlib/*
# é‡æ–°å•Ÿå‹• Spyder

# Mac æ¨™é¡Œä¿®æ­£å¦‚ä¸‹å³å¯æ­£å¸¸é¡¯ç¤ºä¸­æ–‡
# plt.title("2017å¹´10æœˆæ¯å°æ™‚ç¶²è·¯æµé‡")

#plt.xticks() :è¨­å®šxè»¸åˆ»åº¦, e.g. plt.xticks([10,20,30,40,50])
#plt.yticks() :è¨­å®šyè»¸åˆ»åº¦

# color
"""
b: blue
g: green
r: red
c: cyan
m: magenta
y: yellow
k: black
w: white
"""
#https://matplotlib.org/users/colors.html

# line style or marker
"""
'-'	  solid line style
'--'	  dashed line style
'-.'	  dash-dot line style
':'	  dotted line style
'.'	  point marker
','	  pixel marker
'o'	  circle marker
'v'	  triangle_down marker
'^'	  triangle_up marker
'<'	  triangle_left marker
'>'	  triangle_right marker
'1'	  tri_down marker
'2'	  tri_up marker
'3'	  tri_left marker
'4'	  tri_right marker
's'	  square marker
'p'	  pentagon marker
'*'	  star marker
'h'	  hexagon1 marker
'H'	  hexagon2 marker
'+'	  plus marker
'x'	  x marker
'D'	  diamond marker
'd'	  thin_diamond marker
'|'	  vline marker
'_'	  hline marker
"""
# http://matplotlib.org/users/pyplot_tutorial.html

# æ°´é›· vs. å²©çŸ³ è¦–è¦ºåŒ–åˆ†æ

import pandas as pd
import matplotlib.pyplot as plot

target_url = ("https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data")

# è³‡æ–™æœ‰ 208åˆ—, 61è¡Œ
# X: V0-V59
# Y: V60
# ç¬¬61è¡Œ: R: rock  å²©çŸ³
# ç¬¬61è¡Œ: M: mine æ°´é›·
rocksVMines = pd.read_csv(target_url, header=None, prefix="V")
rocksVMines
#          V0      V1      V2      V3      V4  ...     V56     V57     V58     V59  V60
# 0    0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032    R
# 1    0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044    R
# 2    0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078    R
# 3    0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117    R
# 4    0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094    R
# ..      ...     ...     ...     ...     ...  ...     ...     ...     ...     ...  ...
# 203  0.0187  0.0346  0.0168  0.0177  0.0393  ...  0.0065  0.0115  0.0193  0.0157    M
# 204  0.0323  0.0101  0.0298  0.0564  0.0760  ...  0.0034  0.0032  0.0062  0.0067    M
# 205  0.0522  0.0437  0.0180  0.0292  0.0351  ...  0.0140  0.0138  0.0077  0.0031    M
# 206  0.0303  0.0353  0.0490  0.0608  0.0167  ...  0.0034  0.0079  0.0036  0.0048    M
# 207  0.0260  0.0363  0.0136  0.0272  0.0214  ...  0.0040  0.0036  0.0061  0.0115    M

# print head and tail of data frame
print(rocksVMines.head())
rocksVMines.tail()

# print summary of data frame
summary = rocksVMines.describe()
print(summary)
#                V0          V1          V2  ...         V57         V58         V59
# count  208.000000  208.000000  208.000000  ...  208.000000  208.000000  208.000000
# mean     0.029164    0.038437    0.043832  ...    0.007949    0.007941    0.006507
# std      0.022991    0.032960    0.038428  ...    0.006470    0.006181    0.005031
# min      0.001500    0.000600    0.001500  ...    0.000300    0.000100    0.000600
# 25%      0.013350    0.016450    0.018950  ...    0.003600    0.003675    0.003100
# 50%      0.022800    0.030800    0.034300  ...    0.005800    0.006400    0.005300
# 75%      0.035550    0.047950    0.057950  ...    0.010350    0.010325    0.008525
# max      0.137100    0.233900    0.305900  ...    0.044000    0.036400    0.043900

#å¹³è¡Œåº§æ¨™è»¸ Parallel coordinates graph
for i in range(208):
    # assign color based on color based on "M" or "R" labels
    if rocksVMines.iat[i,60] == "M":
        pcolor = "red"
    else:
        pcolor = "blue"

    # plot rows of data as if they were series data
    dataRow = rocksVMines.iloc[i, 0:60]
    plot.rcParams["figure.figsize"] = (20, 15) #(å¯¬, é«˜)
    dataRow.plot(color=pcolor, alpha=0.5)
    # plot.figure(figsize=(16, 16))

plot.xlabel("Attribute Index")
plot.ylabel("Attribute Values")
plot.show()

# æ•£ä½ˆåœ–çŸ©é™£ scatter_matrix{pandas}
import pandas as pd
from pandas.plotting import scatter_matrix
import numpy as np

df = pd.DataFrame(np.random.randn(1000, 5), columns=['a', 'b', 'c', 'd', 'e'])

# diagonal matrix with histogram
scatter_matrix(df)

scatter_matrix(df, alpha=0.2, figsize=(6, 6))

# diagonal matrix with kernel density estimation
scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde')

# æ•£ä½ˆåœ–çŸ©é™£ pairplot {seaborn}
# https://seaborn.pydata.org/
# seaborn-data: https://github.com/mwaskom/seaborn-data

# å®‰è£ seaborn æ¨¡çµ„
import seaborn as sns

sns.set(style="ticks")

df = sns.load_dataset("iris")
df
#      sepal_length  sepal_width  petal_length  petal_width    species
# 0             5.1          3.5           1.4          0.2     setosa
# 1             4.9          3.0           1.4          0.2     setosa
# 2             4.7          3.2           1.3          0.2     setosa
# 3             4.6          3.1           1.5          0.2     setosa
# 4             5.0          3.6           1.4          0.2     setosa
# ..            ...          ...           ...          ...        ...
# 145           6.7          3.0           5.2          2.3  virginica
# 146           6.3          2.5           5.0          1.9  virginica
# 147           6.5          3.0           5.2          2.0  virginica
# 148           6.2          3.4           5.4          2.3  virginica
# 149           5.9          3.0           5.1          1.8  virginica
# [150 rows x 5 columns]

sns.pairplot(df, hue="species")

###
# Question:
# import seaborn as sns 
# --> ImportError: DLL load failed: æ‰¾ä¸åˆ°æŒ‡å®šçš„ç¨‹åºã€‚

# Solution: å¯èƒ½æ˜¯ scipy æˆ–æ˜¯ numpy çš„ç‰ˆæœ¬å•é¡Œ, ç§»é™¤å¾Œé‡æ–°å®‰è£
# conda remove numpy
# conda install numpy
# conda install seaborn
# åŠ æ²¹!!!

##############################
# 09.MySQLå¸¸ç”¨èªæ³•
##############################

# MySQL ä¸‹è¼‰
# https://dev.mysql.com/downloads/
# é¸å– [MySQL Installer for Windows]
# é¸å– [MySQL Installer 8.0.23] --> Windows (x86, 32-bit), MSI Installer [mysql-installer-community-8.0.23.0.msi] 422.4MB
# mysql-installer-community-8.0.23.0.msi
# å®‰è£å®Œæˆæœƒå•Ÿå‹• (1).MySQL Shell (2).MySQL Workbench

# é–‹å•Ÿ MySQL Workbench, ç·´ç¿’ä»¥ä¸‹ MySQLèªæ³•

-- é¡¯ç¤ºç‰ˆæœ¬ï¼Œæ—¥æœŸ
SELECT VERSION(), CURRENT_DATE;

-- ç¾åœ¨æ™‚é–“
SELECT NOW();

-- é¡¯ç¤ºè³‡æ–™åº«
SHOW databases;

-- ä½¿ç”¨å…§å»ºç¯„ä¾‹è³‡æ–™åº«
USE sakila;

-- ç›®å‰ä½¿ç”¨è³‡æ–™åº«
SELECT DATABASE();

-- é¡¯ç¤ºè³‡æ–™è¡¨
SHOW TABLES;

-- è³‡æ–™è¡¨ç¶±è¦
DESCRIBE rental;

-- æŸ¥è©¢è³‡æ–™
SELECT * FROM rental;

-- ç­†æ•¸ç¸½è¨ˆ 16044
SELECt count(*) FROM rental;

-- GROUP BY
SELECT customer_id, count(customer_id) AS rental_count
FROM rental
GROUP BY customer_id;

-- date å‡½æ•¸
SELECT date(rental_date) FROM rental;

##############################
# 10.Pythoné€£çµMySQL
##############################

# example 1 - è³‡æ–™åº«é€£çµ
import mysql.connector

# https://dev.mysql.com/doc/sakila/en/
cnx = mysql.connector.connect(user='root', 
                              password='123456',
                              host='127.0.0.1',
                              database='sakila')
cnx.close()

# example 2 - SELECT ç¯„ä¾‹
import mysql.connector

cnx = mysql.connector.connect(user='root', password='123456', database='sakila')

cursor = cnx.cursor()

query = "SELECT rental_id, rental_date, customer_id FROM rental"

cursor.execute(query)

# å–å‡ºç¬¬1ç­†, tuple
result1 = cursor.fetchone()
print(result1)

# å–å‡ºå‰6ç­†, list
result2 = cursor.fetchmany(6)
print(result2)

# å–å¾—æ‰€æœ‰è³‡æ–™, list
result3 = cursor.fetchall()
print(result3)

# é—œé–‰æ¸¸æ¨™ True
cursor.close()

# é—œé–‰è³‡æ–™åº«é€£çµ
cnx.close()

##############################
# 11.Pythonç‰©ä»¶å°å‘
##############################

# ç¯„ä¾‹1
# å»ºç«‹é¡åˆ¥ class
class MyClass:
  x = 5

# ä½¿ç”¨ ClassName() å¯¦ä½œå¯¦ä¾‹
p1 = MyClass()
print(p1.x)

# ç¯„ä¾‹2
# __init__() å‡½æ•¸è¡¨ç¤ºé¡åˆ¥åˆä½¿åŒ–æ™‚,è©²å‡½æ•¸æœƒè‡ªå‹•åŸ·è¡Œ, å·¦å³ç‚ºäºŒå€‹åº•ç·š
class Person:
  def __init__(self, name, age):
    self.name = name
    self.age = age

p1 = Person("Alan", 20)

p1

print(p1.name)
print(p1.age)  

# ç¯„ä¾‹3
# Inheritance ç¹¼æ‰¿
# çˆ¶é¡åˆ¥(Parent class)æ˜¯çµ¦å…¶ä»–ç¹¼æ‰¿çš„é¡åˆ¥, ç¨±ç‚ºåŸºæœ¬é¡åˆ¥.
# å­é¡åˆ¥(Child class )æ˜¯å¾å¦ä¸€å€‹çˆ¶é¡åˆ¥ç¹¼æ‰¿çš„, ä¹Ÿç¨±ç‚ºè¡ç”Ÿé¡åˆ¥(derived class).
# https://www.w3schools.com/python/python_inheritance.asp

# çˆ¶é¡åˆ¥
class Person:
  def __init__(self, fname, lname):
    self.firstname = fname
    self.lastname = lname

  def printname(self):
    print(self.firstname, self.lastname)

x = Person("ALAN", "LEE")
x.printname()

# class å­é¡åˆ¥(çˆ¶é¡åˆ¥)
class Student(Person):
  pass

x = Student("Mike", "Olsen")
x.printname()

# æ–°å¢ __init__ å‡½æ•¸
# å­é¡åˆ¥çš„__init __()å‡½æ•¸å°‡è¦†è“‹çˆ¶é¡åˆ¥çš„__init __()å‡½æ•¸
class Student(Person):
  def __init__(self, fname, lname):
    Person.__init__(self, fname, lname)

# super - è¡¨ç¤ºä½¿ç”¨çˆ¶é¡åˆ¥çš„æ–¹æ³•
class Student(Person):
  def __init__(self, fname, lname):
    super().__init__(fname, lname)

x = Student("Big", "Data")

x.printname()
# end

##############################
# 12.iPAS - ç§‘ç›®äºŒï¼šè³‡æ–™è™•ç†èˆ‡åˆ†ææ¦‚è«–
##############################

##############################
# 1-1è³‡æ–™çµ„ç¹”èˆ‡æ¸…ç†
##############################

# Label encoding
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# åŒ¯å…¥è³‡æ–™
# https://github.com/rwepa/DataDemo/blob/master/german_credit.csv
df = pd.read_csv("C:/rdata/german_credit.csv")
df
df['Purpose'].describe()

# Purpose é€²è¡Œ label encoding
labelencoder = LabelEncoder()
df['PurposeEncoding'] = labelencoder.fit_transform(df['Purpose'])

df['Purpose'].head()
df['PurposeEncoding'].head()

# One-hot encoding

# åŒ¯å…¥æ¨¡çµ„
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
  
# åŒ¯å…¥è³‡æ–™
df = pd.read_csv("C:/rdata/german_credit.csv")

df['Purpose'].describe()

# one-hot encoding   
columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), ['Purpose'])],
                                      remainder='passthrough')
  
df1 = np.array(columnTransformer.fit_transform(df), dtype = np.str)
df1

##############################
# 1-2.è³‡æ–™æ‘˜è¦èˆ‡å½™ç¸½
##############################

# ç›’é¬šåœ– boxplot - matplotlib å¥—ä»¶

import pandas as pd
import matplotlib.pyplot as plt

# åƒè€ƒRåŒ¯å‡º Cars93.csv
df = pd.read_csv('C:/rdata/Cars93.csv')

fig1, ax1 = plt.subplots()
ax1.set_title('Python - boxplot')
ax1.boxplot(df['Price'])
ax1.set_xlabel('Cars93 dataset')
ax1.set_ylabel('Price')

# ç¾¤çµ„ç›’é¬šåœ– - matplotlib å¥—ä»¶
data = [df[df.Origin == 'USA']['Price'], 
        df[df.Origin == 'non-USA']['Price']]

fig2, ax2 = plt.subplots()
ax2.set_title('Python - boxplot with group')
ax2.boxplot(data)
ax2.set_xticklabels(['USA', 'non-USA'])
ax2.set_xlabel('Origin')
ax2.set_ylabel('Price')

# ç›’é¬šåœ– boxplot - pandas å¥—ä»¶
df.boxplot(column =['Price'], grid = False)

# ç¾¤çµ„ç›’é¬šåœ– - pandas å¥—ä»¶
df.boxplot(by ='Origin', column =['Price'], grid = False)

# ç›’é¬šåœ– boxplot - seaborn å¥—ä»¶
import seaborn as sns

ax = sns.boxplot(x = df["Price"])

ax = sns.boxplot(y = df["Price"])

# ç¾¤çµ„ç›’é¬šåœ– - seaborn å¥—ä»¶
ax = sns.boxplot(x = "Origin", y = "Price", data=df)

# pandasæ’åº
import pandas as pd
import numpy as np

np.random.seed(168)
df = pd.DataFrame(np.random.randn(3,3), columns=list(['x1', 'x2', 'x3']))
df

# éå¢
df.sort_values(by='x1')

# éæ¸›
df.sort_values(by='x1', ascending = False)

# ç¾¤çµ„
import pandas as pd
df = pd.read_csv('C:/rdata/Cars93.csv')

df = df[['Manufacturer', 'Price', 'AirBags', 'Horsepower', 'Origin']]

df.head()

df_AirBags = df.groupby('AirBags')
type(df_AirBags)

print(df_AirBags.groups)

# ç¾¤çµ„ - 2å€‹ç¶­åº¦
df_AirBagsOrigin = df.groupby(['AirBags', 'Origin'])

# ç¾¤çµ„å¤§å°
df_AirBagsOrigin.size()

# ç¯©é¸ç¾¤çµ„
df_AirBags.get_group('Driver & Passenger')

# ç¾¤çµ„ç¸½å’Œ
df_AirBags.sum()

# ç¾¤çµ„å¹³å‡
df_AirBags.mean()

# agg - æ¯è¡Œè¨ˆç®—min
df_AirBags.agg('min')

# agg - æ¯è¡Œè¨ˆç®—max
df_AirBags.agg('max')

# æ‘˜è¦
import pandas as pd
df = pd.read_csv('C:/rdata/Cars93.csv')
df.describe()

# 1-3.å±¬æ€§è½‰æ›èˆ‡èƒå–

# å¥‡ç•°å€¼åˆ†è§£ (Singular Value Decomposition, SVD)
# åƒè€ƒ Jason Brownlee, Basics of Linear Algebra for Machine Learning: Discover the Mathematical Language of Data in Python, 2018.

from numpy import array
from scipy.linalg import svd

# å®šç¾©çŸ©é™£ A:3*2
A = array([[1, 2], [3, 4], [5, 6]])
print(A)

# SVDè¨ˆç®—
U, s, V = svd(A)
print(U)
print(s)
print(V)

##############################
# 2-1.çµ±è¨ˆåˆ†æåŸºç¤
##############################

# Python - å¹³å‡æ•¸ ğœ‡ ä¹‹å€é–“ä¼°è¨ˆ
from scipy.stats import norm
import math
alpha = 0.05
sampleSD = 5
sampleSize = 16
sampleMean = 60
zscore = norm.ppf(alpha/2)*(-1)
zscore

lowerBounr = sampleMean - zscore*sampleSD/math.sqrt(sampleSize)
upperBound = sampleMean + zscore*sampleSD/math.sqrt(sampleSize)
print([lowerBounr, upperBound])

# Python - tæª¢å®š
from scipy import stats
x = stats.norm.rvs(5, size=20)
x

# å›å‚³äºŒå€‹æ•¸å€¼, çµ±è¨ˆé‡èˆ‡på€¼
stats.ttest_1samp(x, 5) # p-value å¾ˆå¤§
stats.ttest_1samp(x, 1) # p-value å¾ˆå°

print('t-statistic = %7.5f, pvalue = %7.5f' % stats.ttest_1samp(x, 5))

'''
%%	åœ¨å­—ä¸²ä¸­é¡¯ç¤º%
%s  å­—ä¸²é¡¯ç¤º
%d	ä»¥10 é€²ä½æ•´æ•¸æ–¹å¼è¼¸å‡º
%f	å°‡æµ®é» æ•¸ä»¥10é€²ä½æ–¹å¼è¼¸å‡º
%e  ç§‘å­¸è¨˜è™Ÿ, ç”¨å°å¯«eè¡¨ç¤º
%E  ç§‘å­¸è¨˜è™Ÿ, ç”¨å¤§å¯«Eè¡¨ç¤º
'''

# .4f è¡¨ç¤ºæ•¸å°æ•¸é»å¾Œä»¥å››æ¨äº”å…¥æ–¹å¼é¡¯ç¤º4ä½å°æ•¸å€¼
print('22/7 = %.4f' % (22/7)) 

# å¡æ–¹æª¢å®š
from scipy.stats import chisquare
chisquare([230,220,450])

##############################
# 2-2.æ¢ç´¢å¼è³‡æ–™åˆ†æèˆ‡éç›£ç£å¼å­¸ç¿’
##############################

# éšå±¤å¼é›†ç¾¤ç†±ç¹ªåœ– Hierarchically Clustered Heatmap
import statsmodels.api as sm
import seaborn as sns

df = sm.datasets.get_rdataset("mtcars", "datasets", cache=True).data
g = sns.clustermap(df, z_score=1)

##############################
# 2-3.ç·šæ€§æ¨¡å‹èˆ‡ç›£ç£å¼å­¸ç¿’
##############################

# å•Ÿå‹•å‡½æ•¸ (Activation function)

# Sigmoid å‡½æ•¸
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1/(1+(np.e**(-x)))

x = np.arange(-6, 6, 0.1)

plt.plot(x, sigmoid(x))
plt.title("Sigmoid function (0~1)")
plt.show()

# ReLU å‡½æ•¸
import numpy as np
import matplotlib.pyplot as plt

def relu(x):
    return np.maximum(0, x)

x = np.arange(-6, 6, 0.1)

plt.plot(x, relu(x))
plt.title("ReLU function, f(x)=max(0,x)")
plt.show()

# Tanh å‡½æ•¸
import numpy as np
import matplotlib.pyplot as plt

def tanh(x):
    return np.tanh(x)

x = np.arange(-6, 6, 0.1)

plt.plot(x, tanh(x))
plt.title("Tanh function (-1 ~ 1)")
plt.show()

# Softmax å‡½æ•¸
import numpy as np

def softmax(x):
    return np.exp(x)/sum(np.exp(x))

x = np.array([1,2,3,4,1,2,3])

y = softmax(x)
print(y)

# å¤šå±¤æ„ŸçŸ¥å™¨ (MLP) â€“  irisç¯„ä¾‹

# conda install -c conda-forge tensorflow
# conda install -c conda-forge keras

# æ­¥é©Ÿ1 è¼‰å…¥å¥—ä»¶
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical

# æ­¥é©Ÿ2 è³‡æ–™é è™•ç†
np.random.seed(7)  # æŒ‡å®šäº‚æ•¸ç¨®å­

# è¼‰å…¥è³‡æ–™é›†
# https://github.com/rwepa/DataDemo/blob/master/iris.csv
df = pd.read_csv("iris.csv")

# one-hot ç·¨ç¢¼
target_mapping = {"setosa": 0,
                  "versicolor": 1,
                  "virginica": 2}

df["Species"] = df["Species"].map(target_mapping)

dataset = df.values # å–å‡ºè³‡æ–™æ¡†çš„å€¼
np.random.shuffle(dataset)  # ä½¿ç”¨äº‚æ•¸æ‰“äº‚è³‡æ–™çš„åˆ—é †åº

# åˆ†å‰²æˆç‰¹å¾µè³‡æ–™(X)å’Œæ¨™ç±¤è³‡æ–™(Y)
X = dataset[:,0:4].astype(float)
Y = to_categorical(dataset[:,4])

# ç‰¹å¾µæ¨™æº–åŒ–
X -= X.mean(axis=0)
X /= X.std(axis=0)

# åˆ†å‰²æˆè¨“ç·´å’Œæ¸¬è©¦è³‡æ–™é›†
X_train, Y_train = X[:120], Y[:120]     # è¨“ç·´è³‡æ–™å‰120ç­†
X_test, Y_test = X[120:], Y[120:]       # æ¸¬è©¦è³‡æ–™å¾Œ30ç­†

# æ­¥é©Ÿ3 å®šç¾©æ¨¡å‹
# å»ºç«‹Kerasçš„Sequentialæ¨¡å‹
# input(4)-->hiden1(6)-->hiden2(6)-->output(3)
# è¼¸å…¥å±¤ 4å€‹ç‰¹å¾µ
# ç¬¬1éš±è—å±¤ 6å€‹ç¥ç¶“å…ƒ
# ç¬¬2éš±è—å±¤ 6å€‹ç¥ç¶“å…ƒ
# è¼¸å‡ºå±¤ 3å€‹ç¥ç¶“å…ƒ

model = Sequential() # å»ºç«‹ Sequential ç‰©ä»¶
model.add(Dense(6, input_shape=(4,), activation="relu"))
model.add(Dense(6, activation="relu"))
model.add(Dense(3, activation="softmax"))
model.summary()   # é¡¯ç¤ºæ¨¡å‹æ‘˜è¦

# dense (Dense)   : 4*6 + 6 = 30
# dense_1 (Dense) : 6*6 + 6 = 42
# dense_2 (Dense) : 6*3 + 3 = 21
# åˆè¨ˆ = 30 + 42 + 21 = 93

# æ­¥é©Ÿ4 ç·¨è­¯æ¨¡å‹

# ç·¨è­¯æ¨¡å‹
# loss æå¤±å‡½æ•¸, optimizer å„ªåŒ–å™¨å³æ¢¯åº¦ä¸‹é™æ³•, metrics è©•ä¼°æ¨™æº–
# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers

model.compile(loss="categorical_crossentropy", 
              optimizer="adam",
              metrics=["accuracy"])

# æ­¥é©Ÿ5 è¨“ç·´æ¨¡å‹

# è¨“ç·´æ¨¡å‹
# epochs è¨“ç·´é€±æœŸ,  batch_size æ‰¹æ¬¡æ¨£æœ¬å¤§å°

print("Training ...")
model.fit(X_train, Y_train, epochs=100, batch_size=5)

# æ­¥é©Ÿ6 è©•ä¼°èˆ‡å„²å­˜æ¨¡å‹

# è©•ä¼°æ¨¡å‹
print("\nTesting ...")
loss, accuracy = model.evaluate(X_test, Y_test)
print("æº–ç¢ºåº¦ = {:.2f}".format(accuracy))

# å„²å­˜ Keras æ¨¡å‹
print("Saving Model: iris.h5 ...")
model.save("iris.h5")

# ä½¿ç”¨å„²å­˜æ¨¡å‹é€²è¡Œé æ¸¬
from tensorflow import keras

model = Sequential()
model = keras.models.load_model("iris.h5")
model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

loss, accuracy = model.evaluate(X_test, Y_test)
print("æ¸¬è©¦è³‡æ–™é›†çš„æº–ç¢ºåº¦ = {:.2f}".format(accuracy))

Y_pred = model.predict_classes(X_test)
print(Y_pred)

Y_target = dataset[:,4][120:].astype(int)
print(Y_target)

##############################
# 13.æ±ºç­–æ¨¹ç¹ªåœ–4ç¨®æ–¹æ³•
##############################

# åŸ·è¡Œç’°å¢ƒ: windows 10 (64-bit) anaconda with spyder 4.2.5
# åƒè€ƒè³‡æ–™ https://scikit-learn.org/stable/modules/tree.html#tree

from sklearn.datasets import load_iris
from sklearn import tree

iris = load_iris()
X, y = iris.data, iris.target
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, y)

##############################
# ç¹ªåœ–æ–¹æ³•1
# ç¹ªåœ–æ–¼ Spyder \ Plots è¦–çª—
##############################

tree.plot_tree(clf)

##############################
# ç¹ªåœ–æ–¹æ³•2
# ç¹ªåœ–æ–¼ IPython è¦–çª—
##############################

# æ­¥é©Ÿ1 å®‰è£ pydotplus
# Anaconda ç’°å¢ƒ
# conda install pydotplus

# æ­¥é©Ÿ2 ç¹ªè£½æ±ºç­–æ¨¹
import io
import pydotplus
dot_data = io.StringIO()
tree.export_graphviz(clf, out_file=dot_data, feature_names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())

# æ­¥é©Ÿ3 åŒ¯å‡ºæ±ºç­–æ¨¹åœ–æª”
graph.write_png('titanic-RWEPA.png')

# æ­¥é©Ÿ4 åŒ¯å…¥æ±ºç­–æ¨¹
from IPython.core.display import Image
Image(filename='titanic-RWEPA.png')

##############################
# ç¹ªåœ–æ–¹æ³•3 graphviz å¥—ä»¶
# è¼¸å‡ºç‚º PDFæª”æ¡ˆ
##############################

# conda install python-graphviz

import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data) 
graph.render("iris") # è¼¸å‡ºç‚º iris.pdf

##############################
# ç¹ªåœ–æ–¹æ³•4 graphviz å¥—ä»¶
# ç¹ªåœ–æ–¼ IPython è¦–çª—, å¡«ä¸Šé¡è‰²
##############################

dot_data = tree.export_graphviz(clf, 
                                out_file=None,
                                feature_names=iris.feature_names,
                                class_names=iris.target_names,
                                filled=True,
                                rounded=True,
                                special_characters=True)

graph = graphviz.Source(dot_data)
graph

##############################
# 14.æ·±åº¦å­¸ç¿’CNN - MNISTç¯„ä¾‹
##############################

# å®‰è£ conda install tensorflow (æˆ–æ˜¯ pip install tensorflow)
# å®‰è£ conda install keras (æˆ–æ˜¯ pip install keras)

# è¼‰å…¥æ¨¡çµ„
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dropout
from keras.utils import to_categorical

# è¨­å®šäº‚æ•¸ç¨®å­
seed = 168
np.random.seed(seed)

# è¼‰å…¥ MNIST è³‡æ–™é›†
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# å°‡åœ–ç‰‡è½‰æ›æˆ 4D å¼µé‡
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype("float32")
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype("float32")

# å› ç‚ºæ˜¯å›ºå®šç¯„åœ, æ‰€ä»¥åŸ·è¡Œæ­£è¦åŒ–, å¾ 0-255 è‡³ 0-1
X_train = X_train / 255
X_test = X_test / 255

# One-hotç·¨ç¢¼
Y_train = to_categorical(Y_train)
Y_test = to_categorical(Y_test)

# å®šç¾©æ¨¡å‹
model = Sequential()
model.add(Conv2D(16, kernel_size=(5, 5), padding="same", input_shape=(28, 28, 1), activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(32, kernel_size=(5, 5), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(10, activation="softmax"))
model.summary()   # é¡¯ç¤ºæ¨¡å‹æ‘˜è¦è³‡è¨Š

# ç·¨è­¯æ¨¡å‹
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# è¨“ç·´æ¨¡å‹(éœ€è¦ä¸€äº›æ™‚é–“...)
history = model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=128, verbose=2)

# è©•ä¼°æ¨¡å‹
print("\nTesting ...")
loss, accuracy = model.evaluate(X_train, Y_train)
print("è¨“ç·´è³‡æ–™é›†çš„æº–ç¢ºåº¦ = {:.2f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, Y_test)
print("æ¸¬è©¦è³‡æ–™é›†çš„æº–ç¢ºåº¦ = {:.2f}".format(accuracy))

# å„²å­˜Kerasæ¨¡å‹
print("Saving Model: mnist.h5 ...")
model.save("mnist.h5")

# é¡¯ç¤ºåœ–è¡¨ä¾†åˆ†ææ¨¡å‹çš„è¨“ç·´éç¨‹
import matplotlib.pyplot as plt

# é¡¯ç¤ºè¨“ç·´å’Œé©—è­‰æå¤±
loss = history.history["loss"]
epochs = range(1, len(loss)+1)
val_loss = history.history["val_loss"]
plt.plot(epochs, loss, "bo-", label="Training Loss")
plt.plot(epochs, val_loss, "ro--", label="Validation Loss")
plt.title("Training and Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# é¡¯ç¤ºè¨“ç·´å’Œé©—è­‰æº–ç¢ºåº¦
# acc = history.history["acc"]
acc = history.history['accuracy']
epochs = range(1, len(acc)+1)
# val_acc = history.history["val_acc"]
val_acc = history.history["val_accuracy"]
plt.plot(epochs, acc, "bo-", label="Training Acc")
plt.plot(epochs, val_acc, "ro--", label="Validation Acc")
plt.title("Training and Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# åƒè€ƒè³‡æ–™: é™³å…å‚‘, TensorFlow èˆ‡ Keras - Python æ·±åº¦å­¸ç¿’æ‡‰ç”¨å¯¦å‹™, æ——æ¨™.

##############################
# 15.Dashè¦–è¦ºåŒ–ç°¡ä»‹
##############################
# åƒè€ƒè³‡æ–™: https://dash.plotly.com/

# Dash çµ„æˆäºŒå¤§å…ƒä»¶: layout, callback

# åŸ·è¡Œ dash ä¸‰å¤§æ–¹æ³•
# (1).å‘½ä»¤æç¤ºåˆ— python app.py
# (2).Spyder é€è¡ŒåŸ·è¡Œ
# (3).Jupyter notebook é€è¡ŒåŸ·è¡Œ

# Part 1.å®‰è£ plotly, dashç­‰å¥—ä»¶. ä½¿ç”¨ Jupyter notebookæ™‚,é ˆå®‰è£ jupyter-dash æ¨¡çµ„.
# conda install -c conda-forge plotly
# conda install -c conda-forge dash
# conda install -c conda-forge jupyter-dash
# conda install -c conda-forge dash-html-components
# conda install -c conda-forge dash-core-components
# conda install -c conda-forge dash-bootstrap-components

"""
# import dash_html_components as html (ä¸å†ä½¿ç”¨)
# import dash_core_components as dcc (ä¸å†ä½¿ç”¨)
from dash import html
from dash import dcc
import dash_bootstrap_components as dbc

# Spyder : http://127.0.0.1:8050/
# Jupyter : http://127.0.0.1:8051/
"""

# ç¯„ä¾‹1 plotly é•·æ¢åœ–
import plotly.graph_objects as go
import plotly.io as pio
#pio.renderers.default = 'svg'
pio.renderers.default = 'browser'

product = ['Product A', 'Product B', 'Product C']
sale = [20, 14, 23]
fig = go.Figure(data=[go.Bar(x=product, y=sale, text=sale, textposition='auto')])
fig.show()

# Part 2.Dash Layout

# ç¯„ä¾‹2 dash é•·æ¢åœ–

# è¼‰å…¥å¥—ä»¶
import dash
import dash_core_components as dcc
import dash_html_components as html
import plotly.express as px
import pandas as pd

# å»ºç«‹ app
app = dash.Dash(__name__)

# ä½¿ç”¨å­—å…¸ç‰©ä»¶å»ºç«‹è³‡æ–™æ¡†è³‡æ–™é›† 6åˆ—*3è¡Œ
df = pd.DataFrame({
    "Fruit": ["Apples", "Oranges", "Bananas", "Apples", "Oranges", "Bananas"],
    "Amount": [4, 1, 2, 2, 4, 5],
    "City": ["Taipei", "Taipei", "Taipei", "Taichung", "Taichung", "Taichung"]
})

# ä½¿ç”¨ plotly.express å»ºç«‹é•·æ¢åœ–
fig = px.bar(df, x="Fruit", y="Amount", color="City", barmode="group")

# å»ºç«‹ dash layout
# children æ˜¯ç¬¬ä¸€å€‹å±¬æ€§, ä¸€èˆ¬æ­¤å±¬æ€§å¯ä»¥çœç•¥.
# html.H1(children='Hello Dash') èˆ‡ html.H1('Hello Dash') ç›¸åŒ.
# children å¯ä»¥æ˜¯å­—ä¸², æ•¸å­—, list.
app.layout = html.Div(children=[
    html.H1(children='Hello Dash'),

    html.Div(children='''
        Dash: A web application framework for your data (ç¶²é æ‡‰ç”¨æ¡†æ¶)
    '''),

    dcc.Graph(
        id='example-graph',
        figure=fig
    )
])

if __name__ == '__main__':
    # app.run_server(debug=True)
    app.run_server(debug=True, use_reloader=False)
# ä½¿ç”¨ç€è¦½å™¨é–‹å•Ÿ http://127.0.0.1:8050/
# å¦‚æœè¨­å®šæœ¬ä¾‹ä¸­çš„ debug=True, å‰‡ç•¶ç¨‹å¼ç¢¼æ›´æ–°æ™‚, ç€è¦½å™¨æœƒè‡ªå‹•æ›´æ–°,åŸ·è¡Œ"hot-reloading"åŠŸèƒ½.

# ç¯„ä¾‹3 å·¦å³ä¸¦æ’é•·æ¢åœ–
import dash_html_components as html
import dash_core_components as dcc
import dash
import dash_core_components
print(dash_core_components.__version__)

# åŠ å…¥ CSS
external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.layout = html.Div(children=[
    html.H1(children='Hello Dash'),

    html.Div('Dash: A web application framework for Python. 2022.1.16'),

    dcc.Graph(
        id='example-graph',
        figure={
            'data': [
                {'x': [1, 2, 3], 'y': [4, 1, 2], 'type': 'bar', 'name': 'ç”·'},
                {'x': [1, 2, 3], 'y': [2, 4, 5], 'type': 'bar', 'name': 'å¥³'},
            ],
            'layout': {
                'title': 'Dash è³‡æ–™è¦–è¦ºåŒ–'
            }
        }
    )
])

if __name__ == '__main__':
    app.run_server(debug=False)

# ç¯„ä¾‹4 ä¸‹æ‹‰å¼é¸å–®é•·æ¢åœ–
import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output
import plotly.graph_objects as go

app = dash.Dash(__name__)

app.layout = html.Div([
    html.P("2022.01.16-é¸å–é¡è‰² Color:"),
    dcc.Dropdown(
        id="dropdown",
        options=[
            {'label': x, 'value': x}
            for x in ['Gold', 'MediumTurquoise', 'LightGreen']
        ],
        value='Gold',
        clearable=False,
    ),
    dcc.Graph(id="graph"),
])

@app.callback(
    Output("graph", "figure"), 
    [Input("dropdown", "value")])
def display_color(color):
    fig = go.Figure(
        data=go.Bar(y=[2, 3, 1], marker_color=color))
    return fig

app.run_server(debug=False)

##############################
# 16.Foliumåœ°ç†è¦–è¦ºåŒ–æ‡‰ç”¨
##############################

# foliumå®˜ç¶² https://python-visualization.github.io/folium/

# folium ç‰¹æ€§:
# Folium æ˜¯ Python åœ°ç†è¦–è¦ºåŒ–æ¨¡çµ„
# Folium æä¾›äº’å‹•å¼çš„æ“ä½œä»‹é¢
# Folium æ¡ç”¨ leaflet JavaScript å‡½å¼åº«, æ“ä½œèˆ‡Ré¡ä¼¼
# Folium åœ°åœ–æœå‹™çµåˆ OpenStreetMap, Mapbox, and Stamen ç­‰å®¢è£½åŒ–åŠŸèƒ½

# å®‰è£ folium
# conda install -c conda-forge folium 

# ä½¿ç”¨ Spyder å¯ä»¥é…åˆ webbrwoser é–‹å•Ÿç€è¦½å™¨, Anaconda å·²å®‰è£ webbrwoser æ¨¡çµ„
# Jupyter-notebook ä¸ç”¨ä½¿ç”¨ webbrwoser æ¨¡çµ„, å¯ä»¥ç›´æ¥é¡¯ç¤º.

# webbrowser å¯ä»¥ç›´æ¥é–‹å•Ÿç¶²é 
webbrowser.open_new_tab('http://rwepa.blogspot.com/')

##############################
# folium - ç¯„ä¾‹1 å°åŒ—101å¤§æ¨“åœ°åœ–
##############################

import folium

import webbrowser

# [ç·¯åº¦ lat, ç¶“åº¦ long]
# è¡—é“åœ–, æ­¤ç‚ºé è¨­å€¼ tiles='OpenStreetMap' 
m = folium.Map(location=[25.033493, 121.564101], zoom_start=18)

m.save("mymap.html")

webbrowser.open_new_tab("mymap.html")

# stamenterrain åœ°å½¢åœ–
m = folium.Map(location=[25.033493, 121.564101], tiles='stamenterrain', zoom_start=13)
m.save("mymap.html")
webbrowser.open_new_tab("mymap.html")

# tiles='stamentoner' é»‘ç™½ç·šæ¢åœ–
m = folium.Map(location=[25.033493, 121.564101], tiles='stamentoner', zoom_start=13)
m.save("mymap.html")
webbrowser.open_new_tab("mymap.html")

# tiles='stamenwatercolor' æ°´æ³¢åœ–
m = folium.Map(location=[25.033493, 121.564101], tiles='stamenwatercolor', zoom_start=13)
m.save("mymap.html")
webbrowser.open_new_tab("mymap.html")

# tiles='cartodbpositron'  
m = folium.Map(location=[25.033493, 121.564101], tiles='cartodbpositron', zoom_start=13)
m.save("mymap.html")
webbrowser.open_new_tab("mymap.html")

##############################
# folium - ç¯„ä¾‹2 å‹•æ…‹æ”¾ç½®æ¨™è¨˜(marker)
##############################
m = folium.Map(location=[23.97555, 120.97361], tiles="Stamen Terrain", zoom_start=8)

tooltip = "Click me!"

folium.Marker(
    location=[23.97555, 120.97361], 
    popup="<i>è‡ºç£åœ°ç†ä¸­å¿ƒç¢‘</i>", 
    tooltip=tooltip,
    icon=folium.Icon(icon="cloud"),
).add_to(m)

folium.Marker(
    location=[25.033493, 121.564101], 
    popup="<b>å°åŒ—101å¤§æ¨“</b>", 
    tooltip=tooltip,
    icon=folium.Icon(color="red", icon="info-sign")
).add_to(m)

# Jupyter-notebook è¼¸å…¥ m å³å¯é¡¯ç¤ºåœ°åœ–
m.save("mymap.html")

webbrowser.open_new_tab("mymap.html")

##############################
# folium - ç¯„ä¾‹3 åŠ å…¥åœ“å½¢æ¨™è¨˜
##############################

m = folium.Map(location=[25.042276, 121.533394], zoom_start=14)

folium.Circle(
    radius=200,
    location=[25.033493, 121.564101],
    popup="å°åŒ—101å¤§æ¨“",
    color="crimson",
    fill=False,
).add_to(m)

folium.CircleMarker(
    radius=100,
    location=[25.04777, 121.51722],    
    popup="å°åŒ—è»Šç«™",
    color="#3186cc",
    fill=True,
    fill_color="#3186cc",
).add_to(m)

m.save("mymap.html")

webbrowser.open_new_tab("mymap.html")

##############################
# folium - ç¯„ä¾‹4 æ»‘é¼ é»é¸åŠ å…¥æ¨™è¨˜
##############################

m = folium.Map(location=[25.033493, 121.564101], zoom_start=14)

folium.Marker([25.04777, 121.51722], popup="å°åŒ—è»Šç«™").add_to(m)

m.add_child(folium.ClickForMarker(popup="Waypoint"))

m.save("mymap.html")

webbrowser.open_new_tab("mymap.html")
##############################
# folium - ç¯„ä¾‹5 æ–°åŒ—å¸‚å…¬å…±è‡ªè¡Œè»Šç§Ÿè³ƒç³»çµ±(YouBike)æ‡‰ç”¨
##############################

# æ–°åŒ—å¸‚å…¬å…±è‡ªè¡Œè»Šç§Ÿè³ƒç³»çµ±(YouBike)
# https://data.gov.tw/dataset/123026

import folium
import webbrowser
import pandas as pd

df = pd.read_csv('C:/Users/user/Downloads/æ–°åŒ—å¸‚å…¬å…±è‡ªè¡Œè»Šç§Ÿè³ƒç³»çµ±(YouBike).csv') # 664*14
df
df.columns
# ['sno', 'sna', 'tot', 'sbi', 'sarea', 'mday', 'lat', 'lng', 'ar', 'sareaen', 'snaen', 'aren', 'bemp', 'act']
# sno(ç«™é»ä»£è™Ÿ)ã€sna(ä¸­æ–‡å ´ç«™åç¨±)ã€tot(å ´ç«™ç¸½åœè»Šæ ¼)ã€sbi(å¯å€Ÿè»Šä½æ•¸)ã€sarea(ä¸­æ–‡å ´ç«™å€åŸŸ)
# mday(è³‡æ–™æ›´æ–°æ™‚é–“)ã€lat(ç·¯åº¦)ã€lng(ç¶“åº¦)ã€ar(ä¸­æ–‡åœ°å€)ã€sareaen(è‹±æ–‡å ´ç«™å€åŸŸ)
# snaen(è‹±æ–‡å ´ç«™åç¨±)ã€aren(è‹±æ–‡åœ°å€)ã€bemp(å¯é‚„ç©ºä½æ•¸)ã€act(å ´ç«™æ˜¯å¦æš«åœç‡Ÿé‹)

# ç¼ºè»Š: å¯å€Ÿè»Šæ¯”ä¾‹è¼ƒå°è€…,å¯å€Ÿè»Šæ¯”ä¾‹=å¯å€Ÿè»Šä½æ•¸ /å ´ç«™ç¸½åœè»Šæ ¼ = sbi/tot
# ç¼ºè»Šæ¯”ä¾‹ = 1- (sbi/tot)
# ç¼ºè»Šç†±é»: å¯å€Ÿè»Šæ¯”ä¾‹ < è‡ªè¨‚ æˆ– å¯å€Ÿè»Šä½æ•¸ < è‡ªè¨‚å€‹æ•¸

df['lackcar'] = 1 - df['sbi']/df['tot']
df['lackcar'].describe()
# count    664.000000
# mean       0.366837
# std        0.216291
# min        0.000000
# 25%        0.198611
# 50%        0.367544
# 75%        0.533333
# max        0.937500

# å–å‡ºç¼ºè»Šæ¯”ä¾‹è¼ƒå¤§çš„å‰10ç­†è³‡æ–™
df_lackcar = df.sort_values(by=['lackcar'], ascending=False)[0:10]

m = folium.Map(location=[25.012839, 121.456308], tiles='cartodbpositron', zoom_start=13)

for index, row in df.iterrows():
    # å°‡è³‡æ–™é»åŠ åˆ°åœ°åœ–ä¸Š
    folium.Circle(
        radius=100,
        location=[row["lat"], row["lng"]],
        popup="{} \n {}".format(row["sna"], row["tot"]),
        color="#555",
        weight=1,
        fill_color="#FFE082",
        fill_opacity=1,

    ).add_to(m)
    
    # åŠ å…¥ç¼ºè»Šæ¨™è¨˜
    if row["lackcar"] == 1:
        folium.Marker(
            location=[row["lat"], row["lng"]],
            tooltip=row['sna'],
            icon=folium.Icon(color='lightred', icon='fa-regular fa-bicycle', prefix='fa')
    ).add_to(m)
# icon list: https://fontawesome.com/icons

# åŠ å…¥æ¨™é¡Œ
loc = 'æ–°åŒ—å¸‚Youbikeç³»çµ±-2022.2.11-@RWPEA'
title_html = '''
             <h3 align="center" style="font-size:16px"><b>{}</b></h3>
             '''.format(loc)   

m.get_root().html.add_child(folium.Element(title_html))

m.save("mymap.html")

webbrowser.open_new_tab("mymap.html")

# 17.Orange3ç°¡ä»‹

# (1).Orange 3 ç‰¹æ€§
# 1.University of Ljubljana, Slovenia, 10 October 1996
# 2.è¦–è¦ºåŒ–ç¨‹å¼è¨­è¨ˆå·¥å…·(Visual Programming Tools)
# 3.åŸ·è¡Œæ›´åŠ å¿«é€Ÿ(C++)èˆ‡è¦–è¦ºåŒ–æ“ä½œ
# 4.æä¾›å¤šç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡çµ„
# 5.é–‹æ”¾åŸå§‹ç¢¼èˆ‡è·¨å¹³å°
# 6.é©ç”¨æ–¼Pythonæ¨¡çµ„
# 7.è³‡æ–™é è™•ç†
# 8.æ¨¡å‹è¨“ç·´
# 9.éƒ¨ç½²

# (2).åƒè€ƒè³‡æº
# å®˜ç¶²     : https://orangedatamining.com/
# å…ƒä»¶     : https://orangedatamining.com/widget-catalog/
# Tutorial : https://orange3.readthedocs.io/projects/orange-data-mining-library/en/latest/  

# (3).Orange 3 å®‰è£

# æ–¹æ³•1 ä½¿ç”¨ Anaconda
# é¸å– ç¨‹å¼é›† \ Anaconda3 \ Anaconda Navigator (anaconda3)
# é¸å– Orange 3 ä¸¦æŒ‰ä¸‹ [Install] ä¸‹è¼‰ä¸¦å®‰è£
# å®‰è£å®Œæˆå¾ŒæŒ‰ä¸‹ [Launch] å•Ÿå‹• Orange3
# æˆ–æ˜¯åœ¨å‘½ä»¤æç¤ºå­—å…ƒè¼¸å…¥ä»¥ä¸‹å­—å…ƒ: python -m Orange.canvas
# æ³¨æ„:æœ¬å®‰è£æ–¹æ³•é ˆä¸€äº›æ™‚é–“

# æ–¹æ³•2 ä¸‹è¼‰ windows å…å®‰è£æª”. è§£å£“ç¸®å¾Œ,å³å¯ä½¿ç”¨, ä¸ç”¨é€²è¡Œå®‰è£ç¨‹åº.
# https://orangedatamining.com/download/#windows 
# Orange3-3.32.0.zip (550MB)
# ä¸‹è¼‰ä¸¦è§£å£“ç¸®zip
# é–‹å•Ÿ Orange åŸ·è¡Œæª”

# æ–¹æ³•3 ä½¿ç”¨ orange3 è™›æ“¬ç’°å¢ƒ
cd C:\Users\user\anaconda3\envs\orange3
python -m Orange.canvas

# (4).Orange3 add-ons (å¤–æ›å…ƒä»¶)
# 1.Orange3-Associate          é—œè¯è¦å‰‡
# 2.Orange3-Bioinformatics     ç”Ÿç‰©è³‡è¨Š
# 3.Orange3-Educational        æ©Ÿå™¨å­¸ç¿’æ•™è‚²ç¤ºç¯„
# 4.Orange3-Explain            é‡è¦ç‰¹å¾µè§£é‡‹
# 5.Orange3-Geo                åœ°ç†è³‡æ–™è¦–è¦ºåŒ–
# 6.Orange3-ImageAnalytics     å½±åƒåˆ†æ
# 7.Orange3-Network            ç¶²è·¯åˆ†æ
# 8.Orange3-Prototypes         åŸå‹
# 9.Orange3-SingleCell         å–®ç´°èƒåˆ†æ
# 10.Orange-Spectroscopy       å…‰è­œåˆ†æ
# 11.Orange3-Text              æ–‡å­—æ¢å‹˜
# 12.Orange3-Textable          æ–‡å­—æ¢å‹˜, ç·¨ç¢¼
# 13.Orange3-Timeseries        æ™‚é–“åºåˆ—
# 14.Orange3-Survival-Analysis å­˜æ´»åˆ†æ
# 15.Orange3-WorldHappiness    ä¸–ç•Œç¤¾æœƒç¶“æ¿ŸæŒ‡æ¨™ (GDP...)

# (5).å®‰è£ Add-ons Associate
# Options \ Add-ons
# Associate æ‰“å‹¾ é€²è¡Œå®‰è£
# æŒ‰ OK é‡æ–°å•Ÿå‹• Orange

# (6).Associate äºŒå¤§å…ƒä»¶:
# Frequent Itemsets é »ç¹é …ç›®é›† 
# Association Rules é—œè¯è¦å‰‡
# è¼¸å…¥Associateçš„è³‡æ–™é ˆç‚º 0-1 encodingæˆ–æ˜¯é¡åˆ¥å‹è³‡æ–™(yes/no)
# milk egg sugar 
# 1    0    1
# 0    1    1
# 0    0    0

##############################
# 18.condaè™›æ“¬ç’°å¢ƒ
##############################

"""
# æª¢è¦–æ‰€æœ‰è™›æ“¬ç’°å¢ƒæ¸…å–®
conda env list

# å•Ÿç”¨ myenv è™›æ“¬ç’°å¢ƒ
conda activate myenv

# é—œé–‰è™›æ“¬ç’°å¢ƒ
conda deactivate

# å»ºç«‹ myenv è™›æ“¬ç’°å¢ƒ
# --name ä¹Ÿå¯ä»¥ä½¿ç”¨ -n
# --file ä¹Ÿå¯ä»¥ä½¿ç”¨ -f
conda create --name myenv

# å»ºç«‹ç‰¹å®š python ç‰ˆæœ¬çš„è™›æ“¬ç’°å¢ƒ
conda create -n myenv python=3.9

# å»ºç«‹ç‰¹å®š scipy æ¨¡çµ„ç‰ˆæœ¬çš„è™›æ“¬ç’°å¢ƒ
conda create -n myenv scipy=1.9.0

# å»ºç«‹ç‰¹å®š python ç‰ˆæœ¬èˆ‡ç‰¹å®š scipy æ¨¡çµ„ç‰ˆæœ¬çš„è™›æ“¬ç’°å¢ƒ
conda create -n myenv python=3.9 scipy=1.9.0 astroid babel

# æª¢è¦– myenv è™›æ“¬ç’°å¢ƒçš„æ¨¡çµ„è³‡è¨Š
conda list -n myenv scipy

# å®‰è£ myenv è™›æ“¬ç’°å¢ƒçš„ spyder æ¨¡çµ„
conda install -n myenv spyder

# å®‰è£ myenv è™›æ“¬ç’°å¢ƒçš„ rdkit æ¨¡çµ„
# rdkit: Open source toolkit for cheminformatics
# https://www.rdkit.org/docs/GettingStartedInPython.html
conda install -c conda-forge -n myenv rdkit

####################
# è¤‡è£½(clone)è™›æ“¬ç’°å¢ƒ
####################

# æ–¹æ³•1.ä½¿ç”¨condaæŒ‡ä»¤è¤‡è£½(clone)è™›æ“¬ç’°å¢ƒçš„ä¸‰å¤§æ­¥é©Ÿ.
# æ­¤æ–¹æ³•å°‡æ–°å¢ç¨‹å¼é›† Jupyter Notebook (newenv1), Reset Spyder Settings (newenv1), Spyder (newenv1)

# æ­¥é©Ÿ1.é¡¯ç¤ºç’°å¢ƒæ¸…å–®
conda env list

# æ­¥é©Ÿ2.å¦‚æœæœ‰ç™»å…¥, å…ˆåŸ·è¡Œç™»å‡ºç’°å¢ƒ
conda deactivate

# æ­¥é©Ÿ3.è¤‡è£½ç’°å¢ƒ
# conda create --name new_env --clone original_env
# new_env      : æ–°çš„ç’°å¢ƒ, æœ¬ä¾‹ä½¿ç”¨ newenv1
# original_env : åŸå§‹è¢«è¤‡è£½çš„ç’°å¢ƒ, æœ¬ä¾‹ä½¿ç”¨ base
conda create --name newenv1 --clone base

# æ–¹æ³•2.ä½¿ç”¨YMLè¤‡è£½(clone)æ–°ç’°å¢ƒçš„äºŒå¤§æ­¥é©Ÿ.
# æ­¤æ–¹æ³•å°‡æ–°å¢ç¨‹å¼é›† Anaconda Navigator (opencv), Anaconda Prompt (opencv), Anaconda Powershell Prompt (opencv)

# æ­¥é©Ÿ1.åŒ¯å‡ºåŸå§‹ç’°å¢ƒYMLçµ„æ…‹æª”
conda activate base
conda env export > environment.yml
conda deactivate

# æ­¥é©Ÿ2.ä¿®æ”¹ environment.yml
# å°‡ environment.yml ç¬¬1è¡Œname: æ›´åç‚ºæ–°ç’°å¢ƒåç¨±, æœ¬ä¾‹å°‡ base æ›´æ”¹ç‚º opencv

# æ­¥é©Ÿ3.ä½¿ç”¨YMLä¸¦å»ºç«‹æ–°ç’°å¢ƒ
conda env create -f environment.yml

# æ­¥é©Ÿ4.ç™»å…¥opencvç’°å¢ƒ
conda activate opencv

# æ­¥é©Ÿ5.å®‰è£opencvæ¨¡çµ„
pip install opencv-python
pip install opencv-contrib-python

# åŒç†, äº¦å¯ä»¥æ–°å»º orange3 ç’°å¢ƒä¸¦å®‰è£ orange3 æ¨¡çµ„
pip install orange3

# ç™»å…¥ orange3 ç’°å¢ƒ
conda activate orange3

# å•Ÿå‹• Orange3
orange-canvas

reference: https://docs.conda.io/projects/conda/en/4.6.0/user-guide/tasks/manage-environments.html#managing-environments
"""

##############################
# 19.ipynbè½‰æ›ç‚ºpdfæª”æ¡ˆ
##############################

# ä½¿ç”¨ Jupyter-notebook, å¦‚æœé ˆå°‡æª”æ¡ˆè½‰æ›ç‚º PDF, å³ File \ Download as \ PDF via HTML (.html)
# é¡¯ç¤ºä»¥ä¸‹éŒ¯èª¤
# nbconvert failed: Pyppeteer is not installed to support Web PDF conversion. Please install `nbconvert[webpdf]` to enable

# åŸ·è¡Œä»¥ä¸‹3å€‹æŒ‡ä»¤å¾Œ, å³å¯è½‰æ›ç‚ºPDFæª”æ¡ˆ
pip install nbconvert
pip install pyppeteer
pyppeteer-install
# end
